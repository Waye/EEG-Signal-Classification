{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6942a3a-5c06-4172-8a90-1edeba272b46",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d68fbd-1ec1-4ed9-9f91-68f975843ec1",
   "metadata": {},
   "source": [
    "## Outlier Removal Before Filtering\n",
    "Why: Removing outliers before bandpass filtering prevents filter artifacts and ensures the filtering process operates on clean data.\n",
    "Method: The remove_outliers function detects outliers using Z-score thresholding and replaces them with interpolated values.\n",
    "\n",
    "## Zero-Phase Filtering\n",
    "Implementation: Replaced lfilter with filtfilt in the butter_bandpass_filter function.\n",
    "Benefit: Zero-phase filtering avoids phase distortions that can be introduced by standard filtering methods, preserving the temporal characteristics of the EEG signals.\n",
    "\n",
    "## Differential Entropy Computation\n",
    "Adjustment: Added a small constant (1e-6) to the variance in compute_DE to prevent mathematical errors due to zero variance.\n",
    "Importance: Ensures that DE values are computed accurately without encountering logarithm of zero.\n",
    "\n",
    "## Data Reshaping to 10-20 System\n",
    "Purpose: Aligns the processed data with the standard 10-20 EEG electrode placement system.\n",
    "Manual Mapping: Channels are manually assigned to positions on an 8x9 grid. This requires careful attention to ensure accurate representation.\n",
    "\n",
    "## Data Segregation\n",
    "Data Handling:\n",
    "\n",
    "- Processes all 24 trials for multiple participants, stacking segment-level DE features and corresponding labels into a unified dataset.\n",
    "Labels are assigned at the segment level based on predefined trial labels.\n",
    "\n",
    "Output Files:\n",
    "\n",
    "- Saves the computed features and labels to .npy files:\n",
    "X_1D.npy and y.npy: Containing raw DE features (62 channels × 5 bands) and labels.\n",
    "X89.npy: Containing the reshaped 8×9×5 DE features aligned with the 10-20 system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68a4d00-6c04-47bb-a202-58a767409769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1_20160518\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 1_20161125\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 1_20161126\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 2_20150915\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 2_20150920\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 2_20151012\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 3_20150919\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 3_20151018\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 3_20151101\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 4_20151111\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 4_20151118\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 4_20151123\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 5_20160406\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 5_20160413\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 5_20160420\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 6_20150507\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 6_20150511\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 6_20150512\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 7_20150715\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 7_20150717\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 7_20150721\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 8_20151103\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 8_20151110\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 8_20151117\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 9_20151028\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 9_20151119\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 9_20151209\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 10_20151014\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 10_20151021\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 10_20151023\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 11_20150916\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 11_20150921\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 11_20151011\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 12_20150725\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 12_20150804\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 12_20150807\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 13_20151115\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 13_20151125\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 13_20161130\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 14_20151205\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 14_20151208\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 14_20151215\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n",
      "processing 15_20150508\n",
      "1-336\n",
      "temp_trial_de: (62, 5, 336)\n",
      "2-190\n",
      "temp_trial_de: (62, 5, 190)\n",
      "3-398\n",
      "temp_trial_de: (62, 5, 398)\n",
      "4-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "5-176\n",
      "temp_trial_de: (62, 5, 176)\n",
      "6-324\n",
      "temp_trial_de: (62, 5, 324)\n",
      "7-306\n",
      "temp_trial_de: (62, 5, 306)\n",
      "8-418\n",
      "temp_trial_de: (62, 5, 418)\n",
      "9-290\n",
      "temp_trial_de: (62, 5, 290)\n",
      "10-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "11-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "12-220\n",
      "temp_trial_de: (62, 5, 220)\n",
      "13-434\n",
      "temp_trial_de: (62, 5, 434)\n",
      "14-338\n",
      "temp_trial_de: (62, 5, 338)\n",
      "15-518\n",
      "temp_trial_de: (62, 5, 518)\n",
      "16-282\n",
      "temp_trial_de: (62, 5, 282)\n",
      "17-136\n",
      "temp_trial_de: (62, 5, 136)\n",
      "18-358\n",
      "temp_trial_de: (62, 5, 358)\n",
      "19-280\n",
      "temp_trial_de: (62, 5, 280)\n",
      "20-96\n",
      "temp_trial_de: (62, 5, 96)\n",
      "21-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "22-224\n",
      "temp_trial_de: (62, 5, 224)\n",
      "23-350\n",
      "temp_trial_de: (62, 5, 350)\n",
      "24-274\n",
      "temp_trial_de: (62, 5, 274)\n",
      "trial_DE shape: (6870, 62, 5)\n",
      "processing 15_20150514\n",
      "1-442\n",
      "temp_trial_de: (62, 5, 442)\n",
      "2-202\n",
      "temp_trial_de: (62, 5, 202)\n",
      "3-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "4-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "5-428\n",
      "temp_trial_de: (62, 5, 428)\n",
      "6-222\n",
      "temp_trial_de: (62, 5, 222)\n",
      "7-278\n",
      "temp_trial_de: (62, 5, 278)\n",
      "8-368\n",
      "temp_trial_de: (62, 5, 368)\n",
      "9-276\n",
      "temp_trial_de: (62, 5, 276)\n",
      "10-166\n",
      "temp_trial_de: (62, 5, 166)\n",
      "11-480\n",
      "temp_trial_de: (62, 5, 480)\n",
      "12-100\n",
      "temp_trial_de: (62, 5, 100)\n",
      "13-292\n",
      "temp_trial_de: (62, 5, 292)\n",
      "14-216\n",
      "temp_trial_de: (62, 5, 216)\n",
      "15-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "16-122\n",
      "temp_trial_de: (62, 5, 122)\n",
      "17-374\n",
      "temp_trial_de: (62, 5, 374)\n",
      "18-392\n",
      "temp_trial_de: (62, 5, 392)\n",
      "19-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "20-86\n",
      "temp_trial_de: (62, 5, 86)\n",
      "21-298\n",
      "temp_trial_de: (62, 5, 298)\n",
      "22-352\n",
      "temp_trial_de: (62, 5, 352)\n",
      "23-196\n",
      "temp_trial_de: (62, 5, 196)\n",
      "24-152\n",
      "temp_trial_de: (62, 5, 152)\n",
      "trial_DE shape: (6728, 62, 5)\n",
      "processing 15_20150527\n",
      "1-340\n",
      "temp_trial_de: (62, 5, 340)\n",
      "2-260\n",
      "temp_trial_de: (62, 5, 260)\n",
      "3-184\n",
      "temp_trial_de: (62, 5, 184)\n",
      "4-364\n",
      "temp_trial_de: (62, 5, 364)\n",
      "5-386\n",
      "temp_trial_de: (62, 5, 386)\n",
      "6-212\n",
      "temp_trial_de: (62, 5, 212)\n",
      "7-516\n",
      "temp_trial_de: (62, 5, 516)\n",
      "8-186\n",
      "temp_trial_de: (62, 5, 186)\n",
      "9-208\n",
      "temp_trial_de: (62, 5, 208)\n",
      "10-128\n",
      "temp_trial_de: (62, 5, 128)\n",
      "11-414\n",
      "temp_trial_de: (62, 5, 414)\n",
      "12-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "13-314\n",
      "temp_trial_de: (62, 5, 314)\n",
      "14-154\n",
      "temp_trial_de: (62, 5, 154)\n",
      "15-230\n",
      "temp_trial_de: (62, 5, 230)\n",
      "16-354\n",
      "temp_trial_de: (62, 5, 354)\n",
      "17-114\n",
      "temp_trial_de: (62, 5, 114)\n",
      "18-140\n",
      "temp_trial_de: (62, 5, 140)\n",
      "19-366\n",
      "temp_trial_de: (62, 5, 366)\n",
      "20-178\n",
      "temp_trial_de: (62, 5, 178)\n",
      "21-318\n",
      "temp_trial_de: (62, 5, 318)\n",
      "22-310\n",
      "temp_trial_de: (62, 5, 310)\n",
      "23-330\n",
      "temp_trial_de: (62, 5, 330)\n",
      "24-312\n",
      "temp_trial_de: (62, 5, 312)\n",
      "trial_DE shape: (6648, 62, 5)\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "## Extract DE features for 5 frequency bands from each channel of the SEED dataset,\n",
    "## and convert the 62-channel data into an 8*9*5 three-dimensional input,\n",
    "## where 8*9 represents the 2D plane after converting the 62 channels, and 5 represents the 5 frequency bands\n",
    "##############\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "# Function to remove outliers from a signal\n",
    "def remove_outliers(signal, method='zscore', threshold=3):\n",
    "    if method == 'zscore':\n",
    "        mean = np.mean(signal)\n",
    "        std = np.std(signal)\n",
    "        z_scores = np.abs((signal - mean) / std)\n",
    "        mask = z_scores < threshold\n",
    "        signal_clean = signal.copy()\n",
    "        # Replace outliers with interpolated values\n",
    "        indices = np.arange(len(signal))\n",
    "        signal_clean[~mask] = np.interp(indices[~mask], indices[mask], signal_clean[mask])\n",
    "    elif method == 'mad':\n",
    "        median = np.median(signal)\n",
    "        mad = np.median(np.abs(signal - median))\n",
    "        mask = np.abs(signal - median) < threshold * mad\n",
    "        signal_clean = signal.copy()\n",
    "        indices = np.arange(len(signal))\n",
    "        signal_clean[~mask] = np.interp(indices[~mask], indices[mask], signal_clean[mask])\n",
    "    else:\n",
    "        raise ValueError(\"Method not recognized.\")\n",
    "    return signal_clean\n",
    "\n",
    "\n",
    "# Function to create bandpass filter coefficients\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs  # Nyquist frequency\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Function to apply bandpass filter to data\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Function to compute Differential Entropy (DE) of a signal\n",
    "def compute_DE(signal):\n",
    "    variance = np.var(signal, ddof=1) + 1e-6  # Add a small epsilon to prevent log(0)\n",
    "    return math.log(2 * math.pi * math.e * variance) / 2\n",
    "\n",
    "# Function to decompose EEG data into different frequency bands and extract DE features\n",
    "# Input: file path, name (shortened name of the participant)\n",
    "def decompose(file, name):\n",
    "    # Load the .mat file containing the EEG data\n",
    "    data = loadmat(file)\n",
    "    frequency = 200  # Sampling rate of the SEED dataset is downsampled to 200Hz\n",
    "\n",
    "    # Label for each of the 24 trials\n",
    "    all_label = [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3,\n",
    "                 2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,2,1,0,3,0,1,3,1,\n",
    "                 1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0,2,3,0,0,2,0,1,0]\n",
    "\n",
    "    # Create empty arrays to store DE features and labels\n",
    "    decomposed_de = np.empty([0, 62, 5])\n",
    "    label = np.array([])\n",
    "\n",
    "    # Loop through all 24 trials in the dataset\n",
    "    for trial in range(24):\n",
    "        # Load the EEG data for the current trial\n",
    "        tmp_trial_signal = data[name + '_eeg' + str(trial + 1)]\n",
    "\n",
    "        # Number of samples per segment (0.5 seconds per segment, with a sampling rate of 200Hz)\n",
    "        num_sample = int(len(tmp_trial_signal[0]) / 100)\n",
    "        print('{}-{}'.format(trial + 1, num_sample))\n",
    "\n",
    "        # Initialize temporary array to store DE features for each channel\n",
    "        temp_de = np.empty([0, num_sample])\n",
    "        # Assign labels for each sample in the current trial\n",
    "        label = np.append(label, [all_label[trial]] * num_sample)\n",
    "\n",
    "        # Loop through each channel (total 62 channels)\n",
    "        for channel in range(62):\n",
    "            trial_signal = tmp_trial_signal[channel]\n",
    "            \n",
    "            # Remove outliers before filtering\n",
    "            trial_signal = remove_outliers(trial_signal, method='zscore', threshold=3)\n",
    "            \n",
    "            # Apply bandpass filters to extract different frequency bands\n",
    "            delta = butter_bandpass_filter(trial_signal, 1, 4, frequency, order=3)\n",
    "            theta = butter_bandpass_filter(trial_signal, 4, 8, frequency, order=3)\n",
    "            alpha = butter_bandpass_filter(trial_signal, 8, 14, frequency, order=3)\n",
    "            beta = butter_bandpass_filter(trial_signal, 14, 31, frequency, order=3)\n",
    "            gamma = butter_bandpass_filter(trial_signal, 31, 51, frequency, order=3)\n",
    "\n",
    "            # Initialize arrays to store DE values for each frequency band\n",
    "            DE_delta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_theta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_alpha = np.zeros(shape=[0], dtype=float)\n",
    "            DE_beta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_gamma = np.zeros(shape=[0], dtype=float)\n",
    "\n",
    "            # Compute DE features for each frequency band in each segment\n",
    "            for index in range(num_sample):\n",
    "                DE_delta = np.append(DE_delta, compute_DE(delta[index * 100:(index + 1) * 100]))\n",
    "                DE_theta = np.append(DE_theta, compute_DE(theta[index * 100:(index + 1) * 100]))\n",
    "                DE_alpha = np.append(DE_alpha, compute_DE(alpha[index * 100:(index + 1) * 100]))\n",
    "                DE_beta = np.append(DE_beta, compute_DE(beta[index * 100:(index + 1) * 100]))\n",
    "                DE_gamma = np.append(DE_gamma, compute_DE(gamma[index * 100:(index + 1) * 100]))\n",
    "\n",
    "            # Stack the DE features for each frequency band\n",
    "            temp_de = np.vstack([temp_de, DE_delta])\n",
    "            temp_de = np.vstack([temp_de, DE_theta])\n",
    "            temp_de = np.vstack([temp_de, DE_alpha])\n",
    "            temp_de = np.vstack([temp_de, DE_beta])\n",
    "            temp_de = np.vstack([temp_de, DE_gamma])\n",
    "\n",
    "        # Reshape the DE features to match the desired format\n",
    "        temp_trial_de = temp_de.reshape(-1, 5, num_sample)\n",
    "        print(\"temp_trial_de:\", temp_trial_de.shape)  # Print the shape of the reshaped DE features\n",
    "        temp_trial_de = temp_trial_de.transpose([2, 0, 1])  # Rearrange dimensions to match desired format\n",
    "        decomposed_de = np.vstack([decomposed_de, temp_trial_de])  # Stack trial data\n",
    "\n",
    "    print(\"trial_DE shape:\", decomposed_de.shape)\n",
    "    return decomposed_de, label\n",
    "\n",
    "\n",
    "# Main script to extract features and save data\n",
    "file_path = 'D:/BigData/SEED_IV/SEED_IV/eeg_raw_data/'\n",
    "\n",
    "# List of participant names and short names used in file naming\n",
    "people_name = ['1_20160518', '1_20161125','1_20161126',\n",
    "               '2_20150915', '2_20150920','2_20151012',\n",
    "               '3_20150919','3_20151018','3_20151101',\n",
    "               '4_20151111', '4_20151118','4_20151123',\n",
    "               '5_20160406', '5_20160413','5_20160420',\n",
    "               '6_20150507','6_20150511','6_20150512',\n",
    "               '7_20150715', '7_20150717','7_20150721',\n",
    "               '8_20151103', '8_20151110','8_20151117',\n",
    "               '9_20151028','9_20151119','9_20151209',\n",
    "               '10_20151014', '10_20151021','10_20151023',\n",
    "               '11_20150916', '11_20150921','11_20151011',\n",
    "               '12_20150725','12_20150804','12_20150807',\n",
    "               '13_20151115', '13_20151125','13_20161130',\n",
    "               '14_20151205', '14_20151208','14_20151215',\n",
    "               '15_20150508','15_20150514','15_20150527']\n",
    "\n",
    "short_name = ['cz', 'cz','cz',\n",
    "              'ha', 'ha', 'ha',\n",
    "              'hql','hql','hql',\n",
    "              'ldy','ldy','ldy',\n",
    "              'ly','ly','ly',\n",
    "              'mhw','mhw','mhw',\n",
    "              'mz','mz','mz',\n",
    "              'qyt','qyt','qyt',\n",
    "              'rx','rx','rx',\n",
    "              'tyc','tyc','tyc',\n",
    "              'whh','whh','whh',\n",
    "              'wll','wll','wll',\n",
    "              'wq','wq','wq',\n",
    "              'zjd','zjd','zjd',\n",
    "              'zjy','zjy','zjy']\n",
    "\n",
    "# Initialize empty arrays for storing the final DE features and labels\n",
    "X = np.empty([0, 62, 5])\n",
    "y = np.empty([0, 1])\n",
    "\n",
    "# Loop through all participants to extract DE features\n",
    "for i in range(len(people_name)):  # Loop through all 45 experiments (15 participants, 3 trials each)\n",
    "    file_name = file_path + people_name[i]\n",
    "    print('processing {}'.format(people_name[i]))\n",
    "    decomposed_de, label = decompose(file_name, short_name[i])  # Extract DE features for each participant\n",
    "    X = np.vstack([X, decomposed_de])  # Stack the features for all participants\n",
    "    y = np.append(y, label)  # Stack the labels for all participants\n",
    "\n",
    "# Save the extracted DE features and labels as .npy files\n",
    "np.save(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/X_1D.npy\", X)\n",
    "np.save(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/y.npy\", y)\n",
    "\n",
    "# Load the saved features and labels\n",
    "X = np.load('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/X_1D.npy')\n",
    "y = np.load('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/y.npy')\n",
    "\n",
    "def reshape(y,X):\n",
    "    # Reshape the 62-channel data into an 8x9 matrix (based on the 10-20 electrode system)\n",
    "    X89 = np.zeros((len(y), 8, 9, 5))  # Create an empty array for the 8x9x5 data\n",
    "    X89[:, 0, 2, :] = X[:, 3, :]  # Assign values to specific positions in the 8x9 matrix\n",
    "    X89[:, 0, 3:6, :] = X[:, 0:3, :]\n",
    "    X89[:, 0, 6, :] = X[:, 4, :]\n",
    "\n",
    "    # Assign values for the middle rows of the 8x9 matrix\n",
    "    for i in range(5):\n",
    "        X89[:, i + 1, :, :] = X[:, 5 + i * 9:5 + (i + 1) * 9, :]\n",
    "\n",
    "    # Assign values for the last two rows of the 8x9 matrix\n",
    "    X89[:, 6, 1:8, :] = X[:, 50:57, :]\n",
    "    X89[:, 7, 2:7, :] = X[:, 57:62, :]\n",
    "    return X89\n",
    "\n",
    "X89 = reshape(y, X)\n",
    "\n",
    "# Save the reshaped 8x9 matrix data\n",
    "np.save(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/X89.npy\", X89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c9c9f-7488-4ccd-829f-f6daa628a85d",
   "metadata": {},
   "source": [
    "## Loading and Reshaping Data\n",
    "\n",
    "**Purpose**:  \n",
    "Load a comprehensive EEG dataset from a single `.npy` file and organize it by participant for subsequent segmentation.\n",
    "\n",
    "**Implementation**:  \n",
    "- The code starts by loading `X89.npy`, a dataset of shape `(203970, 8, 9, 5)`.  \n",
    "- This data is reshaped into `(15, 13598, 8, 9, 5)`, where the first dimension corresponds to the 15 participants.  \n",
    "- By grouping the data in this manner, the EEG signals from each participant are kept together, facilitating participant-specific segment extraction.\n",
    "\n",
    "\n",
    "\n",
    "## Defining Segment Length\n",
    "\n",
    "**Segment Length (t)**:  \n",
    "- Set to `6`, representing a 3-second segment since each time step is 0.5 seconds.  \n",
    "- **Purpose**: Capturing temporal patterns over a 3-second window helps reveal meaningful temporal dependencies in the EEG data.\n",
    "\n",
    "\n",
    "\n",
    "## Segmenting Data into Fixed-Length Sequences\n",
    "\n",
    "**Function**: `segment_data(falx, t, lengths, labels)`  \n",
    "**Purpose**:  \n",
    "- Break down the participant-organized data into fixed-length sequences (`t=6`) and assign labels to each segment based on experimental conditions or trial definitions.\n",
    "\n",
    "**Method**:  \n",
    "1. **Calculate Boundaries**: Uses `np.cumsum(lengths)` to determine the segmentation points for different trials or conditions.  \n",
    "2. **Iterative Extraction**: For each participant, the code iterates through the trials, extracting non-overlapping segments of length `t`.  \n",
    "3. **Label Assignment**: Each extracted segment receives a label from the `labels` list, ensuring that the ground truth classification is maintained at a segment level.  \n",
    "4. **Memory Management**: Pre-allocates a large array for efficiency, then resizes it after segmentation to exclude any unused space.\n",
    "\n",
    "\n",
    "## Defining Trial Lengths and Labels\n",
    "\n",
    "**Purpose**:  \n",
    "- Provide essential metadata (segment lengths and labels) for accurately slicing the EEG data and linking each segment to its corresponding experimental label.\n",
    "\n",
    "**Data**:  \n",
    "- `lengths`: A list detailing the duration (in time steps) of each trial or experimental condition.  \n",
    "- `all_label`: A list assigning a label to each trial, enabling the code to create a labeled dataset for classification tasks.\n",
    "\n",
    "\n",
    "\n",
    "## Segmenting and Saving Data\n",
    "\n",
    "**Process**:  \n",
    "- Calls `segment_data` with the specified `lengths` and `all_label` lists to produce `new_x` and `new_y`:\n",
    "  - `new_x` contains the segmented EEG data in 3D form `(participant, segment, t, 8, 9, 5)`.\n",
    "  - `new_y` contains the corresponding labels for each segment.\n",
    "\n",
    "**Verification**:  \n",
    "- Prints the shapes of `new_x` and `new_y` to confirm that the segmentation and labeling steps were successful.\n",
    "\n",
    "**Saving**:  \n",
    "- Stores the final segmented data and labels into `.npy` files, for example:\n",
    "  - `t6x_89.npy` for segmented data.\n",
    "  - `t6y_89.npy` for labels.\n",
    "  \n",
    "This ensures that the segmented and labeled dataset is readily accessible for downstream analysis or machine learning model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ec51ea-c7b4-4c99-96d7-82a7cdec8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape-(303690, 8, 9, 5)\n",
      "falx shape-(15, 20246, 8, 9, 5)\n",
      "boundaries-[  336   526   924  1184  1360  1684  1990  2408  2698  3036  3136  3356\n",
      "  3790  4128  4646  4928  5064  5422  5702  5798  6022  6246  6596  6870\n",
      "  7312  7514  7792  8084  8512  8734  9012  9380  9656  9822 10302 10402\n",
      " 10694 10910 11262 11384 11758 12150 12514 12600 12898 13250 13446 13598\n",
      " 13938 14198 14382 14746 15132 15344 15860 16046 16254 16382 16796 17126\n",
      " 17440 17594 17824 18178 18292 18432 18798 18976 19294 19604 19934 20246]\n",
      "total_segments-100000\n",
      "0-3348\n",
      "1-3348\n",
      "2-3348\n",
      "3-3348\n",
      "4-3348\n",
      "5-3348\n",
      "6-3348\n",
      "7-3348\n",
      "8-3348\n",
      "9-3348\n",
      "10-3348\n",
      "11-3348\n",
      "12-3348\n",
      "13-3348\n",
      "14-3348\n",
      "new_x shape-(15, 3348, 6, 8, 9, 5)\n",
      "new_y shape-(50220,)\n"
     ]
    }
   ],
   "source": [
    "# segements\n",
    "import numpy as np\n",
    "\n",
    "X89 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/X89.npy\")\n",
    "print('{}-{}'.format('x_train shape', X89.shape))#x_train shape-(203970, 8, 9, 5)\n",
    "img_rows, img_cols, num_chan = 8, 9, 5\n",
    "falx = X89\n",
    "falx = falx.reshape((15, int(X89.shape[0] / 15), img_rows, img_cols, num_chan)) #falx shape-(15, 13598, 8, 9, 5)\n",
    "print('{}-{}'.format('falx shape', falx.shape))\n",
    "t = 6 #(0.5s ->3s segement  6 pieces)\n",
    "\n",
    "def segment_data(falx, t, lengths, labels):\n",
    "  \"\"\"Segments data into fixed-length segments with corresponding labels.\n",
    "\n",
    "  Args:\n",
    "    falx: The input data array.\n",
    "    t: The length of each segment.\n",
    "    lengths: A list of lengths for each segment.\n",
    "    labels: A list of labels corresponding to each segment.\n",
    "\n",
    "  Returns:\n",
    "    new_x: The segmented data array.\n",
    "    new_y: The corresponding label array.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate boundaries from lengths\n",
    "  boundaries = np.cumsum(lengths)\n",
    "  print('{}-{}'.format('boundaries', boundaries))\n",
    "\n",
    "  total_segments = 100000 #pre-allocated number\n",
    "  print('{}-{}'.format('total_segments', total_segments))\n",
    "\n",
    "  # Pre-allocate new_x with correct dimensions\n",
    "  new_x = np.empty([falx.shape[0], total_segments, t, 8, 9, 5])  \n",
    "  new_y = np.array([])\n",
    "\n",
    "  for nb in range(falx.shape[0]):\n",
    "    z = 0\n",
    "    i = 0\n",
    "    for j, bound in enumerate(boundaries):\n",
    "      while i + t <= bound:\n",
    "        # Assign segments directly, taking all 6 time steps at once\n",
    "        new_x[nb, z] = falx[nb, i:i + t]  # Assign to the first t indices of the segment\n",
    "        new_y = np.append(new_y, labels[j])\n",
    "        i = i + t\n",
    "        z = z + 1\n",
    "      i = bound\n",
    "    print('{}-{}'.format(nb, z))\n",
    "    # Resize new_x to exclude empty cells (segments that were not filled)\n",
    "    new_x = new_x[:, :z, :, :, :, :]\n",
    "  return new_x, new_y\n",
    "\n",
    "lengths =[336,190,398,260,176,324,306,418,290,338,100,220,434,338,518,282,136,358,280,96,224,224,350,274,\n",
    "          442, 202, 278,292,428,222,278, 368, 276, 166, 480, 100, 292, 216, 352, 122, 374, 392, 364, 86, 298, 352, 196, 152,\n",
    "          340, 260, 184, 364, 386, 212, 516, 186, 208, 128, 414, 330, 314, 154, 230, 354, 114, 140, 366, 178, 318, 310, 330, 312]\n",
    "all_label = [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3,\n",
    "             2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,2,1,0,3,0,1,3,1,\n",
    "             1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0,2,3,0,0,2,0,1,0]\n",
    "new_x, new_y = segment_data(falx, 6, lengths, all_label)\n",
    "\n",
    "print('{}-{}'.format('new_x shape', new_x.shape))\n",
    "print('{}-{}'.format('new_y shape', new_y.shape))\n",
    "\n",
    "np.save('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t'+str(t)+'x_89.npy', new_x)#new_x shape-(15, 2247, 6, 8, 9, 5)\n",
    "np.save('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t'+str(t)+'y_89.npy', new_y)#new_y shape-(33705,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b12cc-01cc-44f8-bedc-3ccdd0a6e8ff",
   "metadata": {},
   "source": [
    "# Model Traning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb866e02-86ff-4582-928e-56e35513128e",
   "metadata": {},
   "source": [
    "## Objective: Train deep learning models for emotion recognition from EEG data \n",
    "\n",
    "## Data Handling:\n",
    "\n",
    "Loaded and normalized EEG data.\n",
    "Prepared data for input into the model by reshaping and selecting specific frequency bands.\n",
    "## Model Design:\n",
    "\n",
    "We Created three models seperately.\n",
    "Used a softmax output layer for classification into four emotion classes.\n",
    "\n",
    "## Training and Evaluation:\n",
    "\n",
    "Used K-Fold cross-validation to ensure robustness.\n",
    "Employed early stopping and learning rate reduction for efficient training.\n",
    "Evaluated model performance on both test and validation sets.\n",
    "\n",
    "## Results:\n",
    "\n",
    "Calculated mean and standard deviation of accuracies.\n",
    "Saved trained models for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84c779-6f54-43a9-a61d-0bcbcf371ddf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Multi-CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87731dc-468c-4e47-a9d9-4f9347f9f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "falx shape-(15, 3348, 6, 8, 9, 5)\n",
      "y shape-(50220,)\n",
      "one_y categorical shape-(50220, 4)\n",
      "X_train_all shape: (35154, 6, 8, 9, 4)\n",
      "y_train_all shape: (35154, 4)\n",
      "X_test shape: (15066, 6, 8, 9, 4)\n",
      "y_test shape: (15066, 4)\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 14s 30ms/step - loss: 1.3740 - accuracy: 0.2895 - val_loss: 1.3801 - val_accuracy: 0.3085\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.3398 - accuracy: 0.3302 - val_loss: 1.3193 - val_accuracy: 0.3446\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.2782 - accuracy: 0.3866 - val_loss: 1.2611 - val_accuracy: 0.3795\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.2051 - accuracy: 0.4388 - val_loss: 1.1489 - val_accuracy: 0.4777\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 1.1065 - accuracy: 0.5064 - val_loss: 1.0692 - val_accuracy: 0.5291\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 1.0039 - accuracy: 0.5669 - val_loss: 0.9710 - val_accuracy: 0.5907\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.9073 - accuracy: 0.6168 - val_loss: 0.9314 - val_accuracy: 0.6126\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.8312 - accuracy: 0.6563 - val_loss: 0.8229 - val_accuracy: 0.6653\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.7569 - accuracy: 0.6919 - val_loss: 0.8044 - val_accuracy: 0.6646\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.6963 - accuracy: 0.7189 - val_loss: 0.7764 - val_accuracy: 0.6807\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.6376 - accuracy: 0.7430 - val_loss: 0.7007 - val_accuracy: 0.7201\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.5859 - accuracy: 0.7662 - val_loss: 0.7146 - val_accuracy: 0.7221\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.5491 - accuracy: 0.7812 - val_loss: 0.6950 - val_accuracy: 0.7266\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.5081 - accuracy: 0.7997 - val_loss: 0.6684 - val_accuracy: 0.7379\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.4634 - accuracy: 0.8175 - val_loss: 0.6673 - val_accuracy: 0.7484\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.4249 - accuracy: 0.8310 - val_loss: 0.7327 - val_accuracy: 0.7446\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.3972 - accuracy: 0.8445 - val_loss: 0.6789 - val_accuracy: 0.7552\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.3638 - accuracy: 0.8577 - val_loss: 0.6772 - val_accuracy: 0.7675\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.3402 - accuracy: 0.8680 - val_loss: 0.6426 - val_accuracy: 0.7608\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2992 - accuracy: 0.8837 - val_loss: 0.6952 - val_accuracy: 0.7636\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2839 - accuracy: 0.8916 - val_loss: 0.7142 - val_accuracy: 0.7713\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.2563 - accuracy: 0.9003 - val_loss: 0.6963 - val_accuracy: 0.7734\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2346 - accuracy: 0.9092 - val_loss: 0.7234 - val_accuracy: 0.7625\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.2073 - accuracy: 0.9224 - val_loss: 0.7649 - val_accuracy: 0.7660\n",
      "Epoch 25/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1955 - accuracy: 0.9267 - val_loss: 0.7675 - val_accuracy: 0.7777\n",
      "Epoch 26/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1775 - accuracy: 0.9326 - val_loss: 0.7846 - val_accuracy: 0.7658\n",
      "Epoch 27/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1464 - accuracy: 0.9447 - val_loss: 0.9311 - val_accuracy: 0.7521\n",
      "Epoch 28/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1462 - accuracy: 0.9456 - val_loss: 0.8688 - val_accuracy: 0.7669\n",
      "Epoch 29/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.1292 - accuracy: 0.9525 - val_loss: 0.8669 - val_accuracy: 0.7703\n",
      "Fold 1 Test Accuracy: 76.11%\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 8s 28ms/step - loss: 1.3780 - accuracy: 0.2853 - val_loss: 1.3369 - val_accuracy: 0.3328\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.3290 - accuracy: 0.3437 - val_loss: 1.2780 - val_accuracy: 0.3920\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.2632 - accuracy: 0.4014 - val_loss: 1.2179 - val_accuracy: 0.4386\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 1.1587 - accuracy: 0.4761 - val_loss: 1.0951 - val_accuracy: 0.5233\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.0605 - accuracy: 0.5377 - val_loss: 1.0200 - val_accuracy: 0.5705\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.9771 - accuracy: 0.5817 - val_loss: 0.9423 - val_accuracy: 0.6036\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.8828 - accuracy: 0.6307 - val_loss: 0.8674 - val_accuracy: 0.6316\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.7996 - accuracy: 0.6693 - val_loss: 0.8221 - val_accuracy: 0.6686\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.7389 - accuracy: 0.7000 - val_loss: 0.7532 - val_accuracy: 0.6932\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.6780 - accuracy: 0.7252 - val_loss: 0.7400 - val_accuracy: 0.6955\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.6222 - accuracy: 0.7495 - val_loss: 0.7416 - val_accuracy: 0.7020\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.5813 - accuracy: 0.7660 - val_loss: 0.7023 - val_accuracy: 0.7298\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.5236 - accuracy: 0.7921 - val_loss: 0.6758 - val_accuracy: 0.7404\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.4779 - accuracy: 0.8120 - val_loss: 0.6614 - val_accuracy: 0.7451\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.4463 - accuracy: 0.8236 - val_loss: 0.6894 - val_accuracy: 0.7490\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.4002 - accuracy: 0.8451 - val_loss: 0.6912 - val_accuracy: 0.7454\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.3807 - accuracy: 0.8513 - val_loss: 0.7082 - val_accuracy: 0.7508\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.3291 - accuracy: 0.8732 - val_loss: 0.6958 - val_accuracy: 0.7608\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.3092 - accuracy: 0.8801 - val_loss: 0.6980 - val_accuracy: 0.7595\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2742 - accuracy: 0.8941 - val_loss: 0.6888 - val_accuracy: 0.7685\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2505 - accuracy: 0.9054 - val_loss: 0.7336 - val_accuracy: 0.7561\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2256 - accuracy: 0.9141 - val_loss: 0.7607 - val_accuracy: 0.7655\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.1991 - accuracy: 0.9259 - val_loss: 0.7837 - val_accuracy: 0.7693\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.1747 - accuracy: 0.9344 - val_loss: 0.8100 - val_accuracy: 0.7679\n",
      "Fold 2 Test Accuracy: 74.85%\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 8s 29ms/step - loss: 1.3694 - accuracy: 0.2956 - val_loss: 1.3361 - val_accuracy: 0.3583\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 1.3021 - accuracy: 0.3664 - val_loss: 1.2518 - val_accuracy: 0.4312\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.1990 - accuracy: 0.4520 - val_loss: 1.1941 - val_accuracy: 0.4632\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.0943 - accuracy: 0.5206 - val_loss: 1.0396 - val_accuracy: 0.5481\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.9833 - accuracy: 0.5786 - val_loss: 0.9815 - val_accuracy: 0.5766\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.8979 - accuracy: 0.6245 - val_loss: 0.8997 - val_accuracy: 0.6203\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.8155 - accuracy: 0.6589 - val_loss: 0.8223 - val_accuracy: 0.6622\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.7459 - accuracy: 0.6948 - val_loss: 0.7856 - val_accuracy: 0.6727\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.6868 - accuracy: 0.7225 - val_loss: 0.7355 - val_accuracy: 0.7035\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.6293 - accuracy: 0.7467 - val_loss: 0.7358 - val_accuracy: 0.7073\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.5763 - accuracy: 0.7708 - val_loss: 0.7128 - val_accuracy: 0.7244\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.5299 - accuracy: 0.7888 - val_loss: 0.7514 - val_accuracy: 0.7063\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.4899 - accuracy: 0.8052 - val_loss: 0.7104 - val_accuracy: 0.7356\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.4437 - accuracy: 0.8260 - val_loss: 0.6795 - val_accuracy: 0.7463\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.4082 - accuracy: 0.8403 - val_loss: 0.6668 - val_accuracy: 0.7582\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.3660 - accuracy: 0.8541 - val_loss: 0.7216 - val_accuracy: 0.7470\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.3428 - accuracy: 0.8666 - val_loss: 0.6899 - val_accuracy: 0.7626\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2937 - accuracy: 0.8875 - val_loss: 0.7087 - val_accuracy: 0.7545\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.2723 - accuracy: 0.8938 - val_loss: 0.7149 - val_accuracy: 0.7702\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2383 - accuracy: 0.9077 - val_loss: 0.7475 - val_accuracy: 0.7633\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.2245 - accuracy: 0.9143 - val_loss: 0.7497 - val_accuracy: 0.7685\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1979 - accuracy: 0.9262 - val_loss: 0.7719 - val_accuracy: 0.7709\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1635 - accuracy: 0.9392 - val_loss: 0.8821 - val_accuracy: 0.7484\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1612 - accuracy: 0.9399 - val_loss: 0.8175 - val_accuracy: 0.7722\n",
      "Epoch 25/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1436 - accuracy: 0.9472 - val_loss: 0.8448 - val_accuracy: 0.7656\n",
      "Fold 3 Test Accuracy: 75.99%\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 8s 29ms/step - loss: 1.3636 - accuracy: 0.3083 - val_loss: 1.3274 - val_accuracy: 0.3519\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 1.3022 - accuracy: 0.3751 - val_loss: 1.2846 - val_accuracy: 0.3950\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 1.2175 - accuracy: 0.4436 - val_loss: 1.1754 - val_accuracy: 0.4591\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.1089 - accuracy: 0.5092 - val_loss: 1.0682 - val_accuracy: 0.5324\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.9959 - accuracy: 0.5693 - val_loss: 0.9685 - val_accuracy: 0.5881\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.9004 - accuracy: 0.6187 - val_loss: 0.9038 - val_accuracy: 0.6176\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.8213 - accuracy: 0.6607 - val_loss: 0.8299 - val_accuracy: 0.6609\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.7451 - accuracy: 0.6962 - val_loss: 0.7982 - val_accuracy: 0.6753\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.6771 - accuracy: 0.7261 - val_loss: 0.7884 - val_accuracy: 0.6892\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.6250 - accuracy: 0.7513 - val_loss: 0.7408 - val_accuracy: 0.7076\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.5718 - accuracy: 0.7715 - val_loss: 0.6996 - val_accuracy: 0.7291\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.5176 - accuracy: 0.7959 - val_loss: 0.7310 - val_accuracy: 0.7215\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.4776 - accuracy: 0.8121 - val_loss: 0.7077 - val_accuracy: 0.7255\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.4362 - accuracy: 0.8293 - val_loss: 0.7001 - val_accuracy: 0.7404\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.3965 - accuracy: 0.8457 - val_loss: 0.6840 - val_accuracy: 0.7530\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.3585 - accuracy: 0.8595 - val_loss: 0.7076 - val_accuracy: 0.7502\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.3210 - accuracy: 0.8737 - val_loss: 0.7529 - val_accuracy: 0.7495\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2917 - accuracy: 0.8877 - val_loss: 0.6997 - val_accuracy: 0.7640\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2596 - accuracy: 0.8998 - val_loss: 0.7170 - val_accuracy: 0.7685\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2273 - accuracy: 0.9145 - val_loss: 0.7669 - val_accuracy: 0.7609\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2072 - accuracy: 0.9213 - val_loss: 0.7779 - val_accuracy: 0.7716\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1824 - accuracy: 0.9320 - val_loss: 0.8103 - val_accuracy: 0.7601\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1660 - accuracy: 0.9365 - val_loss: 0.8508 - val_accuracy: 0.7576\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1603 - accuracy: 0.9420 - val_loss: 0.8754 - val_accuracy: 0.7548\n",
      "Epoch 25/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.1335 - accuracy: 0.9515 - val_loss: 0.8851 - val_accuracy: 0.7619\n",
      "Fold 4 Test Accuracy: 75.77%\n",
      "Epoch 1/100\n",
      "220/220 [==============================] - 9s 32ms/step - loss: 1.3694 - accuracy: 0.3006 - val_loss: 1.3436 - val_accuracy: 0.3340\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 1.3104 - accuracy: 0.3751 - val_loss: 1.2852 - val_accuracy: 0.3858\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.2423 - accuracy: 0.4234 - val_loss: 1.1831 - val_accuracy: 0.4701\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.1393 - accuracy: 0.4944 - val_loss: 1.1144 - val_accuracy: 0.4987\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 1.0222 - accuracy: 0.5611 - val_loss: 1.0610 - val_accuracy: 0.5240\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.9240 - accuracy: 0.6118 - val_loss: 0.9438 - val_accuracy: 0.6053\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.8437 - accuracy: 0.6505 - val_loss: 0.8445 - val_accuracy: 0.6455\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.7671 - accuracy: 0.6844 - val_loss: 0.7751 - val_accuracy: 0.6862\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.6958 - accuracy: 0.7152 - val_loss: 0.7643 - val_accuracy: 0.6856\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.6380 - accuracy: 0.7437 - val_loss: 0.7045 - val_accuracy: 0.7189\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.5764 - accuracy: 0.7722 - val_loss: 0.7334 - val_accuracy: 0.7100\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.5328 - accuracy: 0.7891 - val_loss: 0.6559 - val_accuracy: 0.7401\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.4874 - accuracy: 0.8057 - val_loss: 0.6659 - val_accuracy: 0.7422\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.4390 - accuracy: 0.8290 - val_loss: 0.6589 - val_accuracy: 0.7518\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 6s 27ms/step - loss: 0.4013 - accuracy: 0.8426 - val_loss: 0.6825 - val_accuracy: 0.7535\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 6s 26ms/step - loss: 0.3746 - accuracy: 0.8559 - val_loss: 0.6948 - val_accuracy: 0.7589\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.3318 - accuracy: 0.8712 - val_loss: 0.6776 - val_accuracy: 0.7627\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.3022 - accuracy: 0.8829 - val_loss: 0.7069 - val_accuracy: 0.7651\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.2716 - accuracy: 0.8963 - val_loss: 0.6657 - val_accuracy: 0.7691\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.2543 - accuracy: 0.9025 - val_loss: 0.6755 - val_accuracy: 0.7754\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 6s 25ms/step - loss: 0.2091 - accuracy: 0.9207 - val_loss: 0.7728 - val_accuracy: 0.7669\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 5s 25ms/step - loss: 0.1820 - accuracy: 0.9317 - val_loss: 0.7658 - val_accuracy: 0.7687\n",
      "Fold 5 Test Accuracy: 73.70%\n",
      "all_Vali: [76.105135679245, 74.85065460205078, 75.9922981262207, 75.7732629776001, 73.70237708091736]\n",
      "Mean Vali: 75.28474569320679\n",
      "Std Vali: 0.9060513023429172\n",
      "Run time: 758.20 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Layer, Lambda\n",
    "from keras.layers import Flatten, Dense, Concatenate, Reshape, LSTM, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "from keras import backend as K\n",
    "import time\n",
    "import datetime\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#==================================Data Loading and Reshaping=====================================\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "img_rows, img_cols, num_chan = 8, 9, 4\n",
    "\n",
    "falx = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t6x_89.npy\")\n",
    "y = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t6y_89.npy\")\n",
    "print('{}-{}'.format('falx shape', falx.shape))\n",
    "print('{}-{}'.format('y shape', y.shape))\n",
    "\n",
    "one_y = to_categorical(y, num_classes)  \n",
    "print('{}-{}'.format('one_y categorical shape', one_y.shape))\n",
    "\n",
    "# Reshape data into segments of length t=6 and select only 4 bands\n",
    "one_falx_1 = falx.reshape((-1, 6, img_rows, img_cols, 5))  \n",
    "one_falx = one_falx_1[:, :, :, :, 1:5]  # Extract only bands 1 to 4\n",
    "\n",
    "#============================= Fixed 30% Test Set Before K-Fold ==============================\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(one_falx, one_y, \n",
    "                                                            test_size=0.3, \n",
    "                                                            random_state=42, \n",
    "                                                            stratify=one_y.argmax(1))\n",
    "\n",
    "print(\"X_train_all shape:\", X_train_all.shape)\n",
    "print(\"y_train_all shape:\", y_train_all.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Define base network\n",
    "def create_base_network(input_dim):\n",
    "    seq = Sequential()\n",
    "    seq.add(Conv2D(64, 5, activation='relu', padding='same', name='conv1', input_shape=input_dim))\n",
    "    seq.add(Conv2D(128, 4, activation='relu', padding='same', name='conv2'))\n",
    "    seq.add(Conv2D(256, 4, activation='relu', padding='same', name='conv3'))\n",
    "    seq.add(Conv2D(64, 1, activation='relu', padding='same', name='conv4'))\n",
    "    seq.add(MaxPooling2D(2, 2, name='pool1'))\n",
    "    seq.add(Flatten(name='fla1'))\n",
    "    seq.add(Dense(512, activation='relu', name='dense1'))\n",
    "    seq.add(Reshape((1, 512), name='reshape'))\n",
    "    return seq\n",
    "\n",
    "acc_list = []\n",
    "std_list = []\n",
    "all_acc = []\n",
    "\n",
    "# Initialize KFold on the training portion (70% of the data)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "K.clear_session()\n",
    "start = time.time()\n",
    "\n",
    "img_size = (img_rows, img_cols, num_chan)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train_all)):\n",
    "    X_fold_train, X_fold_val = X_train_all[train_indices], X_train_all[val_indices]\n",
    "    y_fold_train, y_fold_val = y_train_all[train_indices], y_train_all[val_indices]\n",
    "\n",
    "    base_network = create_base_network(img_size)\n",
    "    input_1 = Input(shape=img_size)\n",
    "    input_2 = Input(shape=img_size)\n",
    "    input_3 = Input(shape=img_size)\n",
    "    input_4 = Input(shape=img_size)\n",
    "    input_5 = Input(shape=img_size)\n",
    "    input_6 = Input(shape=img_size)\n",
    "\n",
    "    out_all = Concatenate(axis=1)([\n",
    "        base_network(input_1), base_network(input_2), base_network(input_3), \n",
    "        base_network(input_4), base_network(input_5), base_network(input_6)\n",
    "    ])\n",
    "    lstm_layer = LSTM(128, name='lstm')(out_all)\n",
    "    out_layer = Dense(4, activation='softmax', name='out')(lstm_layer)\n",
    "    model = Model([input_1, input_2, input_3, input_4, input_5, input_6], out_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # TensorBoard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model on this fold's train/val split\n",
    "    model.fit([\n",
    "       X_fold_train[:, 0, :, :, :], \n",
    "       X_fold_train[:, 1, :, :, :], \n",
    "       X_fold_train[:, 2, :, :, :], \n",
    "       X_fold_train[:, 3, :, :, :], \n",
    "       X_fold_train[:, 4, :, :, :], \n",
    "       X_fold_train[:, 5, :, :, :]], \n",
    "      y_fold_train, epochs=100, batch_size=128, verbose=1,\n",
    "      validation_data=([\n",
    "         X_fold_val[:, 0, :, :, :],\n",
    "         X_fold_val[:, 1, :, :, :],\n",
    "         X_fold_val[:, 2, :, :, :],\n",
    "         X_fold_val[:, 3, :, :, :],\n",
    "         X_fold_val[:, 4, :, :, :],\n",
    "         X_fold_val[:, 5, :, :, :]],\n",
    "         y_fold_val),\n",
    "      callbacks=[early_stopping, tensorboard_callback])\n",
    "\n",
    "    # Evaluate on the fixed 30% test set\n",
    "    scores = model.evaluate([\n",
    "       X_test[:, 0, :, :, :], \n",
    "       X_test[:, 1, :, :, :], \n",
    "       X_test[:, 2, :, :, :], \n",
    "       X_test[:, 3, :, :, :], \n",
    "       X_test[:, 4, :, :, :], \n",
    "       X_test[:, 5, :, :, :]], \n",
    "      y_test, verbose=0)\n",
    "\n",
    "    model.save(f'Multi-CNN-LSTM_fold{fold+1}.h5')\n",
    "    print(f\"Fold {fold+1} Test Accuracy: {scores[1] * 100:.2f}%\")\n",
    "    all_acc.append(scores[1] * 100)\n",
    "\n",
    "print(\"all_Vali:\", all_acc)\n",
    "print(\"Mean Vali:\", np.mean(all_acc))\n",
    "print(\"Std Vali:\", np.std(all_acc))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run time: %.2f seconds\" % (end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314da7c3-459c-4e95-b826-77095add0f47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### output\n",
    "- all_Vali: [76.105135679245, 74.85065460205078, 75.9922981262207, 75.7732629776001, 73.70237708091736]\n",
    "- Mean Vali: 75.28474569320679\n",
    "- Std Vali: 0.9060513023429172"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15640668-41ac-43ef-aac1-ef07c0335046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Attention_BiLSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df3116c-8551-45ce-8a2d-e5a51a19980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "falx shape-(15, 3348, 6, 8, 9, 5)\n",
      "y shape-(50220,)\n",
      "one_y categorical shape-(50220, 4)\n",
      "X_train_all shape: (35154, 6, 8, 9, 4)\n",
      "X_test shape: (15066, 6, 8, 9, 4)\n",
      "y_train_all shape: (35154, 4)\n",
      "y_test shape: (15066, 4)\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 14s 39ms/step - loss: 1.2672 - accuracy: 0.4083 - val_loss: 1.2895 - val_accuracy: 0.4271\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 1.1160 - accuracy: 0.5113 - val_loss: 1.5310 - val_accuracy: 0.4062\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 1.0061 - accuracy: 0.5738 - val_loss: 1.0289 - val_accuracy: 0.5649\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.9155 - accuracy: 0.6166 - val_loss: 0.8955 - val_accuracy: 0.6225\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.8430 - accuracy: 0.6554 - val_loss: 1.0179 - val_accuracy: 0.5895\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.7874 - accuracy: 0.6802 - val_loss: 1.1349 - val_accuracy: 0.5651\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.7305 - accuracy: 0.7050 - val_loss: 0.9594 - val_accuracy: 0.6285\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6764 - accuracy: 0.7300 - val_loss: 0.6971 - val_accuracy: 0.7208\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6431 - accuracy: 0.7432 - val_loss: 0.7676 - val_accuracy: 0.6982\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6076 - accuracy: 0.7579 - val_loss: 0.8114 - val_accuracy: 0.6924\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5728 - accuracy: 0.7743 - val_loss: 0.6714 - val_accuracy: 0.7387\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5423 - accuracy: 0.7868 - val_loss: 0.6256 - val_accuracy: 0.7548\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5200 - accuracy: 0.7975 - val_loss: 0.7206 - val_accuracy: 0.7221\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.4944 - accuracy: 0.8069 - val_loss: 0.6464 - val_accuracy: 0.7557\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 8s 35ms/step - loss: 0.4676 - accuracy: 0.8169 - val_loss: 0.7410 - val_accuracy: 0.7154\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 8s 35ms/step - loss: 0.4443 - accuracy: 0.8248 - val_loss: 0.5890 - val_accuracy: 0.7702\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 8s 35ms/step - loss: 0.4250 - accuracy: 0.8350 - val_loss: 0.6102 - val_accuracy: 0.7729\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 8s 34ms/step - loss: 0.4075 - accuracy: 0.8416 - val_loss: 0.6062 - val_accuracy: 0.7702\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3905 - accuracy: 0.8492 - val_loss: 0.5796 - val_accuracy: 0.7823\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3731 - accuracy: 0.8555 - val_loss: 0.5703 - val_accuracy: 0.7898\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3608 - accuracy: 0.8608 - val_loss: 0.7130 - val_accuracy: 0.7400\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3475 - accuracy: 0.8668 - val_loss: 0.5421 - val_accuracy: 0.7975\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3279 - accuracy: 0.8760 - val_loss: 0.5680 - val_accuracy: 0.7919\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3204 - accuracy: 0.8762 - val_loss: 0.5908 - val_accuracy: 0.7811\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3047 - accuracy: 0.8853 - val_loss: 0.5626 - val_accuracy: 0.7925\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2971 - accuracy: 0.8875 - val_loss: 0.6576 - val_accuracy: 0.7576\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2898 - accuracy: 0.8894 - val_loss: 0.5817 - val_accuracy: 0.7844\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2801 - accuracy: 0.8932 - val_loss: 0.5342 - val_accuracy: 0.8061\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2725 - accuracy: 0.8967 - val_loss: 0.5847 - val_accuracy: 0.7923\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2643 - accuracy: 0.8978 - val_loss: 0.5834 - val_accuracy: 0.7972\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2425 - accuracy: 0.9089 - val_loss: 0.7373 - val_accuracy: 0.7609\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2389 - accuracy: 0.9105 - val_loss: 0.6296 - val_accuracy: 0.7859\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2380 - accuracy: 0.9105 - val_loss: 0.5896 - val_accuracy: 0.7970\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2286 - accuracy: 0.9144 - val_loss: 0.5146 - val_accuracy: 0.8154\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2209 - accuracy: 0.9167 - val_loss: 0.5672 - val_accuracy: 0.8087\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2186 - accuracy: 0.9203 - val_loss: 0.6183 - val_accuracy: 0.7936\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2104 - accuracy: 0.9212 - val_loss: 0.5287 - val_accuracy: 0.8152\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2096 - accuracy: 0.9222 - val_loss: 0.6140 - val_accuracy: 0.8030\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.1970 - accuracy: 0.9276 - val_loss: 0.5739 - val_accuracy: 0.8057\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 8s 34ms/step - loss: 0.1901 - accuracy: 0.9293 - val_loss: 0.5956 - val_accuracy: 0.8012\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.1904 - accuracy: 0.9287 - val_loss: 0.6974 - val_accuracy: 0.7710\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1839 - accuracy: 0.9323 - val_loss: 0.5673 - val_accuracy: 0.8195\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1871 - accuracy: 0.9312 - val_loss: 0.5498 - val_accuracy: 0.8214\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1848 - accuracy: 0.9296 - val_loss: 0.6221 - val_accuracy: 0.8024\n",
      "Fold 1 Test Accuracy: 81.64%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 12s 37ms/step - loss: 1.2675 - accuracy: 0.4063 - val_loss: 1.4078 - val_accuracy: 0.3952\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 1.1155 - accuracy: 0.5108 - val_loss: 1.2003 - val_accuracy: 0.4940\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 1.0067 - accuracy: 0.5727 - val_loss: 0.9678 - val_accuracy: 0.5934\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.9190 - accuracy: 0.6143 - val_loss: 1.0070 - val_accuracy: 0.5733\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.8448 - accuracy: 0.6525 - val_loss: 0.9649 - val_accuracy: 0.6100\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.7847 - accuracy: 0.6815 - val_loss: 0.9625 - val_accuracy: 0.6139\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.7287 - accuracy: 0.7048 - val_loss: 0.9421 - val_accuracy: 0.6232\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6815 - accuracy: 0.7282 - val_loss: 0.8124 - val_accuracy: 0.6781\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6427 - accuracy: 0.7424 - val_loss: 0.7653 - val_accuracy: 0.6928\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6110 - accuracy: 0.7588 - val_loss: 0.8331 - val_accuracy: 0.6753\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.5763 - accuracy: 0.7717 - val_loss: 0.6803 - val_accuracy: 0.7333\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.5466 - accuracy: 0.7866 - val_loss: 0.6184 - val_accuracy: 0.7581\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.5161 - accuracy: 0.7980 - val_loss: 0.6776 - val_accuracy: 0.7319\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.5042 - accuracy: 0.8031 - val_loss: 0.7202 - val_accuracy: 0.7274\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.4797 - accuracy: 0.8121 - val_loss: 0.7573 - val_accuracy: 0.7212\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4472 - accuracy: 0.8256 - val_loss: 0.6270 - val_accuracy: 0.7584\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4349 - accuracy: 0.8300 - val_loss: 0.5711 - val_accuracy: 0.7850\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.4201 - accuracy: 0.8385 - val_loss: 0.6361 - val_accuracy: 0.7622\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.4009 - accuracy: 0.8484 - val_loss: 0.5981 - val_accuracy: 0.7727\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 8s 34ms/step - loss: 0.3819 - accuracy: 0.8536 - val_loss: 0.6612 - val_accuracy: 0.7594\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3744 - accuracy: 0.8544 - val_loss: 0.6665 - val_accuracy: 0.7524\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3534 - accuracy: 0.8649 - val_loss: 0.5808 - val_accuracy: 0.7894\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3396 - accuracy: 0.8702 - val_loss: 0.6310 - val_accuracy: 0.7630\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3325 - accuracy: 0.8724 - val_loss: 0.5942 - val_accuracy: 0.7821\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3206 - accuracy: 0.8764 - val_loss: 0.6333 - val_accuracy: 0.7803\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.3137 - accuracy: 0.8791 - val_loss: 0.5791 - val_accuracy: 0.7875\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2961 - accuracy: 0.8877 - val_loss: 0.6110 - val_accuracy: 0.7794\n",
      "Fold 2 Test Accuracy: 78.30%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 12s 37ms/step - loss: 1.2673 - accuracy: 0.4086 - val_loss: 1.1848 - val_accuracy: 0.4769\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 1.1116 - accuracy: 0.5120 - val_loss: 1.0943 - val_accuracy: 0.5315\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.9991 - accuracy: 0.5732 - val_loss: 0.9963 - val_accuracy: 0.5757\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 8s 34ms/step - loss: 0.9037 - accuracy: 0.6253 - val_loss: 0.9882 - val_accuracy: 0.5827\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 8s 35ms/step - loss: 0.8419 - accuracy: 0.6522 - val_loss: 0.9163 - val_accuracy: 0.6143\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.7753 - accuracy: 0.6854 - val_loss: 0.8357 - val_accuracy: 0.6606\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.7281 - accuracy: 0.7058 - val_loss: 0.8483 - val_accuracy: 0.6514\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6801 - accuracy: 0.7282 - val_loss: 0.7536 - val_accuracy: 0.7039\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6393 - accuracy: 0.7448 - val_loss: 0.6985 - val_accuracy: 0.7202\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6071 - accuracy: 0.7562 - val_loss: 0.7650 - val_accuracy: 0.6959\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5742 - accuracy: 0.7707 - val_loss: 0.6644 - val_accuracy: 0.7360\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.5453 - accuracy: 0.7855 - val_loss: 0.6759 - val_accuracy: 0.7367\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.5136 - accuracy: 0.7994 - val_loss: 0.7142 - val_accuracy: 0.7278\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5003 - accuracy: 0.8044 - val_loss: 0.6385 - val_accuracy: 0.7518\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4680 - accuracy: 0.8161 - val_loss: 0.6466 - val_accuracy: 0.7484\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4476 - accuracy: 0.8252 - val_loss: 0.5885 - val_accuracy: 0.7764\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 8s 34ms/step - loss: 0.4279 - accuracy: 0.8346 - val_loss: 0.5870 - val_accuracy: 0.7810\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4191 - accuracy: 0.8363 - val_loss: 0.5748 - val_accuracy: 0.7813\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3973 - accuracy: 0.8476 - val_loss: 0.6645 - val_accuracy: 0.7585\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3766 - accuracy: 0.8542 - val_loss: 0.7380 - val_accuracy: 0.7275\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3667 - accuracy: 0.8573 - val_loss: 0.5826 - val_accuracy: 0.7791\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3494 - accuracy: 0.8666 - val_loss: 0.5859 - val_accuracy: 0.7783\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3430 - accuracy: 0.8691 - val_loss: 0.6226 - val_accuracy: 0.7652\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3226 - accuracy: 0.8770 - val_loss: 0.5911 - val_accuracy: 0.7865\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3166 - accuracy: 0.8788 - val_loss: 0.5456 - val_accuracy: 0.8051\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3051 - accuracy: 0.8845 - val_loss: 0.5270 - val_accuracy: 0.8069\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2977 - accuracy: 0.8869 - val_loss: 0.5686 - val_accuracy: 0.7911\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2829 - accuracy: 0.8939 - val_loss: 0.5306 - val_accuracy: 0.8094\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2752 - accuracy: 0.8958 - val_loss: 0.6626 - val_accuracy: 0.7724\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2666 - accuracy: 0.9001 - val_loss: 0.5373 - val_accuracy: 0.8168\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2587 - accuracy: 0.9025 - val_loss: 0.5259 - val_accuracy: 0.8140\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2497 - accuracy: 0.9057 - val_loss: 0.5404 - val_accuracy: 0.8106\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2408 - accuracy: 0.9098 - val_loss: 0.7625 - val_accuracy: 0.7549\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2307 - accuracy: 0.9144 - val_loss: 0.4977 - val_accuracy: 0.8253\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2254 - accuracy: 0.9153 - val_loss: 0.5614 - val_accuracy: 0.8144\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2238 - accuracy: 0.9176 - val_loss: 0.6383 - val_accuracy: 0.7921\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2145 - accuracy: 0.9201 - val_loss: 0.5515 - val_accuracy: 0.8110\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2107 - accuracy: 0.9222 - val_loss: 0.6311 - val_accuracy: 0.7925\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2031 - accuracy: 0.9236 - val_loss: 0.5184 - val_accuracy: 0.8302\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.1986 - accuracy: 0.9261 - val_loss: 0.6630 - val_accuracy: 0.7871\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.1927 - accuracy: 0.9268 - val_loss: 0.7334 - val_accuracy: 0.7786\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 8s 34ms/step - loss: 0.1922 - accuracy: 0.9276 - val_loss: 0.5414 - val_accuracy: 0.8293\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.1848 - accuracy: 0.9334 - val_loss: 0.5329 - val_accuracy: 0.8215\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.1747 - accuracy: 0.9372 - val_loss: 0.5345 - val_accuracy: 0.8275\n",
      "Fold 3 Test Accuracy: 82.49%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 12s 37ms/step - loss: 1.2657 - accuracy: 0.4082 - val_loss: 1.3087 - val_accuracy: 0.4100\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 1.1159 - accuracy: 0.5110 - val_loss: 1.1179 - val_accuracy: 0.5097\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 1.0020 - accuracy: 0.5746 - val_loss: 1.1960 - val_accuracy: 0.4839\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.9082 - accuracy: 0.6223 - val_loss: 1.0338 - val_accuracy: 0.5733\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.8386 - accuracy: 0.6577 - val_loss: 0.9522 - val_accuracy: 0.6102\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.7751 - accuracy: 0.6839 - val_loss: 0.8577 - val_accuracy: 0.6578\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.7283 - accuracy: 0.7054 - val_loss: 0.8193 - val_accuracy: 0.6639\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6725 - accuracy: 0.7294 - val_loss: 0.7783 - val_accuracy: 0.6902\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6340 - accuracy: 0.7470 - val_loss: 0.7196 - val_accuracy: 0.7181\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5957 - accuracy: 0.7656 - val_loss: 0.6667 - val_accuracy: 0.7360\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5573 - accuracy: 0.7800 - val_loss: 0.7413 - val_accuracy: 0.7161\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 8s 35ms/step - loss: 0.5404 - accuracy: 0.7862 - val_loss: 0.6259 - val_accuracy: 0.7530\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5038 - accuracy: 0.8046 - val_loss: 0.6276 - val_accuracy: 0.7558\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4827 - accuracy: 0.8104 - val_loss: 0.6223 - val_accuracy: 0.7650\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4602 - accuracy: 0.8187 - val_loss: 0.6456 - val_accuracy: 0.7532\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4395 - accuracy: 0.8302 - val_loss: 0.6901 - val_accuracy: 0.7419\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4206 - accuracy: 0.8376 - val_loss: 0.6119 - val_accuracy: 0.7719\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4020 - accuracy: 0.8446 - val_loss: 0.5961 - val_accuracy: 0.7795\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3899 - accuracy: 0.8487 - val_loss: 0.7435 - val_accuracy: 0.7309\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3675 - accuracy: 0.8601 - val_loss: 0.6155 - val_accuracy: 0.7703\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3534 - accuracy: 0.8669 - val_loss: 0.6122 - val_accuracy: 0.7753\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3418 - accuracy: 0.8698 - val_loss: 0.5921 - val_accuracy: 0.7879\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3309 - accuracy: 0.8742 - val_loss: 0.6143 - val_accuracy: 0.7776\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3164 - accuracy: 0.8798 - val_loss: 0.5923 - val_accuracy: 0.7888\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3041 - accuracy: 0.8857 - val_loss: 0.5358 - val_accuracy: 0.8034\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2940 - accuracy: 0.8879 - val_loss: 0.5786 - val_accuracy: 0.7916\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2840 - accuracy: 0.8924 - val_loss: 0.6092 - val_accuracy: 0.7895\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2767 - accuracy: 0.8944 - val_loss: 0.5492 - val_accuracy: 0.8043\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2691 - accuracy: 0.8994 - val_loss: 0.5476 - val_accuracy: 0.8073\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2651 - accuracy: 0.8987 - val_loss: 0.5713 - val_accuracy: 0.8015\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2568 - accuracy: 0.9028 - val_loss: 0.5376 - val_accuracy: 0.8141\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2426 - accuracy: 0.9092 - val_loss: 0.5537 - val_accuracy: 0.8086\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2356 - accuracy: 0.9114 - val_loss: 0.5464 - val_accuracy: 0.8103\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2275 - accuracy: 0.9161 - val_loss: 0.5758 - val_accuracy: 0.8047\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2248 - accuracy: 0.9160 - val_loss: 0.6786 - val_accuracy: 0.7786\n",
      "Fold 4 Test Accuracy: 81.24%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 13s 38ms/step - loss: 1.2665 - accuracy: 0.4128 - val_loss: 1.2729 - val_accuracy: 0.4331\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 1.1219 - accuracy: 0.5062 - val_loss: 1.0854 - val_accuracy: 0.5356\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 1.0160 - accuracy: 0.5638 - val_loss: 1.1283 - val_accuracy: 0.5132\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.9229 - accuracy: 0.6121 - val_loss: 0.9591 - val_accuracy: 0.5949\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.8593 - accuracy: 0.6466 - val_loss: 0.9734 - val_accuracy: 0.5846\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.7935 - accuracy: 0.6761 - val_loss: 0.8061 - val_accuracy: 0.6676\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.7395 - accuracy: 0.6971 - val_loss: 0.8791 - val_accuracy: 0.6420\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.6895 - accuracy: 0.7242 - val_loss: 0.7521 - val_accuracy: 0.6910\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6452 - accuracy: 0.7423 - val_loss: 0.7985 - val_accuracy: 0.6808\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.6130 - accuracy: 0.7567 - val_loss: 0.6997 - val_accuracy: 0.7230\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5741 - accuracy: 0.7735 - val_loss: 0.6550 - val_accuracy: 0.7371\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.5498 - accuracy: 0.7846 - val_loss: 0.7264 - val_accuracy: 0.7183\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.5255 - accuracy: 0.7937 - val_loss: 0.5874 - val_accuracy: 0.7690\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4961 - accuracy: 0.8072 - val_loss: 0.5956 - val_accuracy: 0.7661\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.4720 - accuracy: 0.8138 - val_loss: 0.6945 - val_accuracy: 0.7380\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.4513 - accuracy: 0.8249 - val_loss: 0.6732 - val_accuracy: 0.7418\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.4303 - accuracy: 0.8332 - val_loss: 0.6701 - val_accuracy: 0.7455\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.4080 - accuracy: 0.8426 - val_loss: 0.6029 - val_accuracy: 0.7697\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.3943 - accuracy: 0.8492 - val_loss: 0.5875 - val_accuracy: 0.7787\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3789 - accuracy: 0.8532 - val_loss: 0.5933 - val_accuracy: 0.7771\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3594 - accuracy: 0.8622 - val_loss: 0.5567 - val_accuracy: 0.7960\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3475 - accuracy: 0.8679 - val_loss: 0.6460 - val_accuracy: 0.7671\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3323 - accuracy: 0.8748 - val_loss: 0.5617 - val_accuracy: 0.7912\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3302 - accuracy: 0.8722 - val_loss: 0.5948 - val_accuracy: 0.7824\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3198 - accuracy: 0.8773 - val_loss: 0.5622 - val_accuracy: 0.7982\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.3021 - accuracy: 0.8841 - val_loss: 0.6030 - val_accuracy: 0.7883\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2916 - accuracy: 0.8890 - val_loss: 0.5607 - val_accuracy: 0.7937\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2814 - accuracy: 0.8947 - val_loss: 0.5309 - val_accuracy: 0.8111\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2738 - accuracy: 0.8969 - val_loss: 0.5907 - val_accuracy: 0.7910\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2611 - accuracy: 0.9002 - val_loss: 0.5836 - val_accuracy: 0.7943\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2517 - accuracy: 0.9046 - val_loss: 0.5534 - val_accuracy: 0.8091\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2477 - accuracy: 0.9072 - val_loss: 0.5782 - val_accuracy: 0.8003\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2396 - accuracy: 0.9109 - val_loss: 0.5598 - val_accuracy: 0.8060\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2378 - accuracy: 0.9119 - val_loss: 0.6119 - val_accuracy: 0.7959\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.2290 - accuracy: 0.9141 - val_loss: 0.6792 - val_accuracy: 0.7737\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2174 - accuracy: 0.9197 - val_loss: 0.6152 - val_accuracy: 0.7980\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2173 - accuracy: 0.9193 - val_loss: 0.6207 - val_accuracy: 0.7956\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.2169 - accuracy: 0.9195 - val_loss: 0.5146 - val_accuracy: 0.8229\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.2009 - accuracy: 0.9269 - val_loss: 0.5176 - val_accuracy: 0.8262\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 8s 35ms/step - loss: 0.1954 - accuracy: 0.9282 - val_loss: 0.5431 - val_accuracy: 0.8220\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1914 - accuracy: 0.9282 - val_loss: 0.7527 - val_accuracy: 0.7640\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1852 - accuracy: 0.9324 - val_loss: 0.5365 - val_accuracy: 0.8246\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1871 - accuracy: 0.9287 - val_loss: 0.5313 - val_accuracy: 0.8321\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1843 - accuracy: 0.9313 - val_loss: 0.7062 - val_accuracy: 0.7802\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1744 - accuracy: 0.9350 - val_loss: 0.5908 - val_accuracy: 0.8135\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 7s 32ms/step - loss: 0.1745 - accuracy: 0.9364 - val_loss: 0.5694 - val_accuracy: 0.8201\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 7s 33ms/step - loss: 0.1692 - accuracy: 0.9385 - val_loss: 0.5579 - val_accuracy: 0.8218\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 7s 34ms/step - loss: 0.1619 - accuracy: 0.9401 - val_loss: 0.5450 - val_accuracy: 0.8266\n",
      "Fold 5 Test Accuracy: 81.81%\n",
      "all_Vali: [81.64077997207642, 78.30213904380798, 82.49037861824036, 81.24253153800964, 81.8067193031311]\n",
      "Mean Vali: 81.0965096950531\n",
      "Std Vali: 1.4541889912968695\n",
      "Run time: 0:24:54.566074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Layer, Lambda\n",
    "from keras.layers import Flatten, Dense, Concatenate, Reshape, LSTM, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "#==================================Data Loading and reshaping=====================================\n",
    "num_classes = 4\n",
    "batch_size = 128\n",
    "img_rows, img_cols, num_chan = 8, 9, 4\n",
    "\n",
    "falx = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t6x_89.npy\")\n",
    "y = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t6y_89.npy\")\n",
    "print('{}-{}'.format('falx shape', falx.shape))  # e.g. (N, 6, 8, 9, 5)\n",
    "print('{}-{}'.format('y shape', y.shape))\n",
    "\n",
    "one_y = to_categorical(y, num_classes)  \n",
    "print('{}-{}'.format('one_y categorical shape', one_y.shape))\n",
    "\n",
    "one_falx_1 = falx.reshape((-1, 6, img_rows, img_cols, 5))  # reshape into segments of length t=6\n",
    "one_falx = one_falx_1[:, :, :, :, 1:5]  # only 4 bands (excluding the first band if it's a \"sleep\" feature)\n",
    "\n",
    "#==================================30% Test Split Before K-Fold====================================\n",
    "\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(one_falx, one_y, \n",
    "                                                            test_size=0.3, \n",
    "                                                            random_state=42,\n",
    "                                                            stratify=one_y.argmax(1))\n",
    "\n",
    "print(\"X_train_all shape:\", X_train_all.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train_all shape:\", y_train_all.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# After this split:\n",
    "# - X_test and y_test remain fixed for final evaluation (30% of entire data)\n",
    "# - X_train_all and y_train_all (70%) are used for K-Fold\n",
    "\n",
    "# Define the AttentionLayer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer='zero', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (None, 6, 256)\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)  # shape: (None, 6, 256)\n",
    "        \n",
    "        # Apply attention mechanism (just an example, ensure correctness)\n",
    "        e_sum = K.sum(e, axis=-1)  # shape: (None, 6)\n",
    "        a = K.softmax(e_sum)       # shape: (None, 6)\n",
    "        \n",
    "        # Weighted sum of the inputs by attention weights\n",
    "        a_expanded = K.expand_dims(a, axis=-1)  # shape: (None, 6, 1)\n",
    "        output = x * a_expanded\n",
    "        return K.sum(output, axis=1)  # shape: (None, 256)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "#def base-network\n",
    "def create_base_network(input_dim):\n",
    "    seq = Sequential()\n",
    "    seq.add(Conv2D(32, 5, activation='relu', padding='same', name='conv1', input_shape=input_dim))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(0.2))\n",
    "    seq.add(Conv2D(128, 4, activation='relu', padding='same', name='conv2'))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(0.2))\n",
    "    seq.add(Conv2D(256, 4, activation='relu', padding='same', name='conv3'))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(0.2))\n",
    "    seq.add(Conv2D(64, 1, activation='relu', padding='same', name='conv4'))\n",
    "    seq.add(MaxPooling2D(2, 2, name='pool1'))\n",
    "    seq.add(Flatten(name='fla1'))\n",
    "    seq.add(Dense(256, activation='relu', name='dense1'))\n",
    "    seq.add(BatchNormalization())\n",
    "    seq.add(Dropout(0.3))\n",
    "    seq.add(Reshape((1, 256), name='reshape'))\n",
    "    return seq\n",
    "\n",
    "#====================================Training with K-Fold on the 70% training set=================================\n",
    "\n",
    "acc_list = []\n",
    "std_list = []\n",
    "all_acc = []\n",
    "\n",
    "# Initialize KFold with the desired number of splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "K.clear_session()\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "img_size = (img_rows, img_cols, num_chan)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train_all)):\n",
    "    X_fold_train, X_fold_val = X_train_all[train_indices], X_train_all[val_indices]\n",
    "    y_fold_train, y_fold_val = y_train_all[train_indices], y_train_all[val_indices]\n",
    "\n",
    "    # Create the model\n",
    "    base_network = create_base_network(img_size)\n",
    "    input_1 = Input(shape=img_size)\n",
    "    input_2 = Input(shape=img_size)\n",
    "    input_3 = Input(shape=img_size)\n",
    "    input_4 = Input(shape=img_size)\n",
    "    input_5 = Input(shape=img_size)\n",
    "    input_6 = Input(shape=img_size)\n",
    "\n",
    "    out_all = Concatenate(axis=1)([\n",
    "        base_network(input_1), \n",
    "        base_network(input_2), \n",
    "        base_network(input_3), \n",
    "        base_network(input_4), \n",
    "        base_network(input_5), \n",
    "        base_network(input_6)\n",
    "    ])\n",
    "    # Apply attention\n",
    "    attention_output = AttentionLayer()(out_all)\n",
    "    lstm_layer = Bidirectional(LSTM(128, name='lstm'))(out_all)\n",
    "    out_layer = Dense(4, activation='softmax', name='out')(lstm_layer)\n",
    "    model = Model([input_1, input_2, input_3, input_4, input_5, input_6], out_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # TensorBoard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Fit the model on the fold's train/val splits\n",
    "    model.fit([\n",
    "        X_fold_train[:, 0, :, :, :], \n",
    "        X_fold_train[:, 1, :, :, :], \n",
    "        X_fold_train[:, 2, :, :, :], \n",
    "        X_fold_train[:, 3, :, :, :], \n",
    "        X_fold_train[:, 4, :, :, :], \n",
    "        X_fold_train[:, 5, :, :, :]], \n",
    "        y_fold_train, epochs=150, batch_size=batch_size, verbose=1,\n",
    "        validation_data=([\n",
    "            X_fold_val[:, 0, :, :, :],\n",
    "            X_fold_val[:, 1, :, :, :],\n",
    "            X_fold_val[:, 2, :, :, :],\n",
    "            X_fold_val[:, 3, :, :, :],\n",
    "            X_fold_val[:, 4, :, :, :],\n",
    "            X_fold_val[:, 5, :, :, :]],\n",
    "            y_fold_val),\n",
    "        callbacks=[early_stopping, tensorboard_callback])\n",
    "\n",
    "    # Evaluate on the fixed 30% test set\n",
    "    scores = model.evaluate([\n",
    "        X_test[:, 0, :, :, :], \n",
    "        X_test[:, 1, :, :, :], \n",
    "        X_test[:, 2, :, :, :], \n",
    "        X_test[:, 3, :, :, :], \n",
    "        X_test[:, 4, :, :, :], \n",
    "        X_test[:, 5, :, :, :]], \n",
    "        y_test, verbose=0)\n",
    "    \n",
    "    model.save(f'Attention-BiLSTM-CNN_fold{fold+1}.h5')\n",
    "    print(f\"Fold {fold+1} Test Accuracy: {scores[1]*100:.2f}%\")\n",
    "    all_acc.append(scores[1] * 100)\n",
    "\n",
    "print(\"all_Vali:\", all_acc)\n",
    "print(\"Mean Vali:\", np.mean(all_acc))\n",
    "print(\"Std Vali:\", np.std(all_acc))\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "run_time = end - start\n",
    "print(\"Run time:\", run_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40c36f-82a9-438d-98dd-a04620749a3a",
   "metadata": {},
   "source": [
    "### output\n",
    "- all_Vali: [81.64077997207642, 78.30213904380798, 82.49037861824036, 81.24253153800964, 81.8067193031311]\n",
    "- Mean Vali: 81.0965096950531\n",
    "- Std Vali: 1.4541889912968695"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa75020-d3b2-4444-968b-8e6a9581a798",
   "metadata": {},
   "source": [
    "## ResCNN-TransGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30f0a85-9242-490b-b0ef-175ad422a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "falx shape-(15, 3348, 6, 8, 9, 5)\n",
      "y shape-(50220,)\n",
      "one_y categorical shape-(50220, 4)\n",
      "X_train_all shape: (35154, 6, 8, 9, 4)\n",
      "y_train_all shape: (35154, 4)\n",
      "X_test shape: (15066, 6, 8, 9, 4)\n",
      "y_test shape: (15066, 4)\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 20s 63ms/step - loss: 1.3023 - accuracy: 0.3950 - val_loss: 1.2863 - val_accuracy: 0.4416 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 1.1473 - accuracy: 0.4917 - val_loss: 1.1225 - val_accuracy: 0.5230 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 12s 57ms/step - loss: 1.0459 - accuracy: 0.5530 - val_loss: 1.1641 - val_accuracy: 0.5053 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 12s 57ms/step - loss: 0.9756 - accuracy: 0.5843 - val_loss: 0.9687 - val_accuracy: 0.5867 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 0.9039 - accuracy: 0.6215 - val_loss: 0.9496 - val_accuracy: 0.5989 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 13s 57ms/step - loss: 0.8492 - accuracy: 0.6503 - val_loss: 0.9047 - val_accuracy: 0.6302 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 13s 57ms/step - loss: 0.8038 - accuracy: 0.6708 - val_loss: 1.0199 - val_accuracy: 0.5867 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 13s 57ms/step - loss: 0.7506 - accuracy: 0.6942 - val_loss: 0.7828 - val_accuracy: 0.6841 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 13s 57ms/step - loss: 0.7113 - accuracy: 0.7134 - val_loss: 0.9015 - val_accuracy: 0.6544 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 0.6752 - accuracy: 0.7292 - val_loss: 0.7153 - val_accuracy: 0.7200 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 0.6399 - accuracy: 0.7455 - val_loss: 0.7397 - val_accuracy: 0.7052 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 12s 57ms/step - loss: 0.6087 - accuracy: 0.7606 - val_loss: 0.6963 - val_accuracy: 0.7269 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 12s 57ms/step - loss: 0.5809 - accuracy: 0.7698 - val_loss: 0.6495 - val_accuracy: 0.7491 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 12s 57ms/step - loss: 0.5607 - accuracy: 0.7773 - val_loss: 0.7330 - val_accuracy: 0.7104 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 13s 61ms/step - loss: 0.5427 - accuracy: 0.7876 - val_loss: 0.6429 - val_accuracy: 0.7458 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.5194 - accuracy: 0.7972 - val_loss: 0.6000 - val_accuracy: 0.7690 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 19s 88ms/step - loss: 0.4927 - accuracy: 0.8082 - val_loss: 0.6290 - val_accuracy: 0.7549 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.4739 - accuracy: 0.8170 - val_loss: 0.6163 - val_accuracy: 0.7618 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.4581 - accuracy: 0.8229 - val_loss: 0.6371 - val_accuracy: 0.7591 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 0.4413 - accuracy: 0.8276 - val_loss: 0.5802 - val_accuracy: 0.7848 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 0.4301 - accuracy: 0.8331 - val_loss: 0.6149 - val_accuracy: 0.7675 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 16s 73ms/step - loss: 0.4066 - accuracy: 0.8434 - val_loss: 0.6062 - val_accuracy: 0.7723 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.3964 - accuracy: 0.8476 - val_loss: 0.5695 - val_accuracy: 0.7847 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.3893 - accuracy: 0.8475 - val_loss: 0.5597 - val_accuracy: 0.7908 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.3745 - accuracy: 0.8545 - val_loss: 0.5334 - val_accuracy: 0.7966 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 17s 77ms/step - loss: 0.3652 - accuracy: 0.8594 - val_loss: 0.5282 - val_accuracy: 0.8057 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.3426 - accuracy: 0.8678 - val_loss: 0.5559 - val_accuracy: 0.7975 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.3332 - accuracy: 0.8722 - val_loss: 0.5740 - val_accuracy: 0.7894 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.3281 - accuracy: 0.8728 - val_loss: 0.5613 - val_accuracy: 0.7942 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.3174 - accuracy: 0.8788 - val_loss: 0.5974 - val_accuracy: 0.7848 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8832\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.3049 - accuracy: 0.8832 - val_loss: 0.6745 - val_accuracy: 0.7658 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2047 - accuracy: 0.9235 - val_loss: 0.4094 - val_accuracy: 0.8488 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.1816 - accuracy: 0.9348 - val_loss: 0.4047 - val_accuracy: 0.8545 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.1678 - accuracy: 0.9392 - val_loss: 0.4065 - val_accuracy: 0.8564 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 21s 98ms/step - loss: 0.1608 - accuracy: 0.9430 - val_loss: 0.4174 - val_accuracy: 0.8539 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1559 - accuracy: 0.9428 - val_loss: 0.4210 - val_accuracy: 0.8495 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1477 - accuracy: 0.9466 - val_loss: 0.4172 - val_accuracy: 0.8596 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9466\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.1452 - accuracy: 0.9466 - val_loss: 0.4159 - val_accuracy: 0.8568 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 21s 95ms/step - loss: 0.1281 - accuracy: 0.9557 - val_loss: 0.4086 - val_accuracy: 0.8609 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1248 - accuracy: 0.9551 - val_loss: 0.4068 - val_accuracy: 0.8616 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1215 - accuracy: 0.9574 - val_loss: 0.4058 - val_accuracy: 0.8616 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.1166 - accuracy: 0.9606 - val_loss: 0.4052 - val_accuracy: 0.8612 - lr: 2.0000e-05\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9577\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.1197 - accuracy: 0.9577 - val_loss: 0.4065 - val_accuracy: 0.8613 - lr: 2.0000e-05\n",
      "Fold 1 Test Accuracy: 85.44%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 43s 116ms/step - loss: 1.3081 - accuracy: 0.3842 - val_loss: 1.2381 - val_accuracy: 0.4474 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 1.1559 - accuracy: 0.4901 - val_loss: 1.1409 - val_accuracy: 0.5041 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 1.0647 - accuracy: 0.5369 - val_loss: 1.0445 - val_accuracy: 0.5439 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.9828 - accuracy: 0.5790 - val_loss: 1.0832 - val_accuracy: 0.5493 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.9241 - accuracy: 0.6126 - val_loss: 0.9333 - val_accuracy: 0.6040 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 21s 95ms/step - loss: 0.8619 - accuracy: 0.6454 - val_loss: 0.9276 - val_accuracy: 0.6119 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.8188 - accuracy: 0.6669 - val_loss: 0.8166 - val_accuracy: 0.6592 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.7752 - accuracy: 0.6847 - val_loss: 0.8079 - val_accuracy: 0.6734 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.7335 - accuracy: 0.7018 - val_loss: 0.8843 - val_accuracy: 0.6399 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.6998 - accuracy: 0.7162 - val_loss: 0.9215 - val_accuracy: 0.6325 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.6625 - accuracy: 0.7321 - val_loss: 0.8477 - val_accuracy: 0.6642 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.6325 - accuracy: 0.7481 - val_loss: 0.7829 - val_accuracy: 0.6945 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 16s 72ms/step - loss: 0.6076 - accuracy: 0.7594 - val_loss: 0.7225 - val_accuracy: 0.7124 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.5744 - accuracy: 0.7737 - val_loss: 0.7788 - val_accuracy: 0.6945 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.5536 - accuracy: 0.7789 - val_loss: 0.6799 - val_accuracy: 0.7346 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.5266 - accuracy: 0.7922 - val_loss: 0.6866 - val_accuracy: 0.7320 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.5123 - accuracy: 0.7956 - val_loss: 0.7548 - val_accuracy: 0.7117 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 21s 93ms/step - loss: 0.4948 - accuracy: 0.8061 - val_loss: 0.6733 - val_accuracy: 0.7434 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.4729 - accuracy: 0.8158 - val_loss: 0.6351 - val_accuracy: 0.7579 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.4596 - accuracy: 0.8212 - val_loss: 0.6118 - val_accuracy: 0.7643 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.4401 - accuracy: 0.8300 - val_loss: 0.5598 - val_accuracy: 0.7906 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 24s 107ms/step - loss: 0.4230 - accuracy: 0.8368 - val_loss: 0.5938 - val_accuracy: 0.7693 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.4098 - accuracy: 0.8429 - val_loss: 0.5725 - val_accuracy: 0.7780 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.3964 - accuracy: 0.8456 - val_loss: 0.7145 - val_accuracy: 0.7306 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.3939 - accuracy: 0.8470 - val_loss: 0.6058 - val_accuracy: 0.7733 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 24s 107ms/step - loss: 0.3726 - accuracy: 0.8568 - val_loss: 0.5078 - val_accuracy: 0.8073 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 23s 107ms/step - loss: 0.3563 - accuracy: 0.8645 - val_loss: 0.5747 - val_accuracy: 0.7864 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.3554 - accuracy: 0.8628 - val_loss: 0.6145 - val_accuracy: 0.7729 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.3403 - accuracy: 0.8703 - val_loss: 0.5532 - val_accuracy: 0.7905 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.3261 - accuracy: 0.8738 - val_loss: 0.6062 - val_accuracy: 0.7783 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.8755\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.3224 - accuracy: 0.8755 - val_loss: 0.5519 - val_accuracy: 0.8013 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.2175 - accuracy: 0.9191 - val_loss: 0.4318 - val_accuracy: 0.8448 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.1925 - accuracy: 0.9298 - val_loss: 0.4238 - val_accuracy: 0.8461 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 21s 95ms/step - loss: 0.1815 - accuracy: 0.9338 - val_loss: 0.4420 - val_accuracy: 0.8460 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.1752 - accuracy: 0.9350 - val_loss: 0.4313 - val_accuracy: 0.8509 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.1678 - accuracy: 0.9376 - val_loss: 0.4420 - val_accuracy: 0.8491 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 24s 109ms/step - loss: 0.1637 - accuracy: 0.9402 - val_loss: 0.4487 - val_accuracy: 0.8484 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9414\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "220/220 [==============================] - 24s 107ms/step - loss: 0.1596 - accuracy: 0.9414 - val_loss: 0.4370 - val_accuracy: 0.8498 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 24s 111ms/step - loss: 0.1408 - accuracy: 0.9505 - val_loss: 0.4192 - val_accuracy: 0.8565 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 24s 111ms/step - loss: 0.1333 - accuracy: 0.9531 - val_loss: 0.4171 - val_accuracy: 0.8572 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - 24s 107ms/step - loss: 0.1337 - accuracy: 0.9522 - val_loss: 0.4211 - val_accuracy: 0.8571 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.1304 - accuracy: 0.9531 - val_loss: 0.4169 - val_accuracy: 0.8588 - lr: 2.0000e-05\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.1294 - accuracy: 0.9535 - val_loss: 0.4219 - val_accuracy: 0.8548 - lr: 2.0000e-05\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.1291 - accuracy: 0.9544 - val_loss: 0.4164 - val_accuracy: 0.8586 - lr: 2.0000e-05\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1304 - accuracy: 0.9536 - val_loss: 0.4262 - val_accuracy: 0.8566 - lr: 2.0000e-05\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.1244 - accuracy: 0.9559 - val_loss: 0.4222 - val_accuracy: 0.8596 - lr: 2.0000e-05\n",
      "Epoch 47/150\n",
      "220/220 [==============================] - 23s 102ms/step - loss: 0.1235 - accuracy: 0.9566 - val_loss: 0.4264 - val_accuracy: 0.8568 - lr: 2.0000e-05\n",
      "Epoch 48/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1252 - accuracy: 0.9548 - val_loss: 0.4242 - val_accuracy: 0.8593 - lr: 2.0000e-05\n",
      "Epoch 49/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9575\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.1204 - accuracy: 0.9575 - val_loss: 0.4223 - val_accuracy: 0.8581 - lr: 2.0000e-05\n",
      "Epoch 50/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.1204 - accuracy: 0.9580 - val_loss: 0.4220 - val_accuracy: 0.8609 - lr: 4.0000e-06\n",
      "Epoch 51/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.1169 - accuracy: 0.9584 - val_loss: 0.4229 - val_accuracy: 0.8610 - lr: 4.0000e-06\n",
      "Epoch 52/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1180 - accuracy: 0.9580 - val_loss: 0.4233 - val_accuracy: 0.8589 - lr: 4.0000e-06\n",
      "Epoch 53/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.1185 - accuracy: 0.9577 - val_loss: 0.4215 - val_accuracy: 0.8585 - lr: 4.0000e-06\n",
      "Epoch 54/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9599\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.1179 - accuracy: 0.9599 - val_loss: 0.4265 - val_accuracy: 0.8586 - lr: 4.0000e-06\n",
      "Fold 2 Test Accuracy: 85.74%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 43s 121ms/step - loss: 1.3099 - accuracy: 0.3849 - val_loss: 1.2984 - val_accuracy: 0.4072 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 1.1579 - accuracy: 0.4855 - val_loss: 1.2346 - val_accuracy: 0.4608 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 25s 112ms/step - loss: 1.0665 - accuracy: 0.5373 - val_loss: 1.0532 - val_accuracy: 0.5440 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.9845 - accuracy: 0.5849 - val_loss: 1.0038 - val_accuracy: 0.5607 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 24s 109ms/step - loss: 0.9220 - accuracy: 0.6165 - val_loss: 1.0222 - val_accuracy: 0.5727 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.8662 - accuracy: 0.6446 - val_loss: 1.0895 - val_accuracy: 0.5676 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 21s 95ms/step - loss: 0.8249 - accuracy: 0.6621 - val_loss: 1.0097 - val_accuracy: 0.6038 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.7744 - accuracy: 0.6852 - val_loss: 0.8529 - val_accuracy: 0.6507 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.7293 - accuracy: 0.7034 - val_loss: 0.8080 - val_accuracy: 0.6810 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.6964 - accuracy: 0.7199 - val_loss: 0.7380 - val_accuracy: 0.6972 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.6708 - accuracy: 0.7309 - val_loss: 0.7937 - val_accuracy: 0.6834 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.6454 - accuracy: 0.7423 - val_loss: 0.7129 - val_accuracy: 0.7137 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.6097 - accuracy: 0.7563 - val_loss: 0.7341 - val_accuracy: 0.7155 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.5904 - accuracy: 0.7692 - val_loss: 0.7184 - val_accuracy: 0.7081 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.5644 - accuracy: 0.7770 - val_loss: 0.6335 - val_accuracy: 0.7541 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.5440 - accuracy: 0.7861 - val_loss: 0.6345 - val_accuracy: 0.7514 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.5221 - accuracy: 0.7971 - val_loss: 0.6290 - val_accuracy: 0.7555 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.4988 - accuracy: 0.8024 - val_loss: 0.6195 - val_accuracy: 0.7592 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.4833 - accuracy: 0.8111 - val_loss: 0.6130 - val_accuracy: 0.7613 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.4630 - accuracy: 0.8214 - val_loss: 0.7068 - val_accuracy: 0.7286 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 23s 107ms/step - loss: 0.4456 - accuracy: 0.8239 - val_loss: 0.6085 - val_accuracy: 0.7643 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.4367 - accuracy: 0.8310 - val_loss: 0.5663 - val_accuracy: 0.7847 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.4213 - accuracy: 0.8357 - val_loss: 0.6384 - val_accuracy: 0.7626 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 24s 109ms/step - loss: 0.4146 - accuracy: 0.8389 - val_loss: 0.7447 - val_accuracy: 0.7231 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.3990 - accuracy: 0.8464 - val_loss: 0.6500 - val_accuracy: 0.7626 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.3848 - accuracy: 0.8525 - val_loss: 0.5758 - val_accuracy: 0.7887 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.8556\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.3687 - accuracy: 0.8556 - val_loss: 0.5973 - val_accuracy: 0.7780 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.2596 - accuracy: 0.9023 - val_loss: 0.4378 - val_accuracy: 0.8396 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 21s 94ms/step - loss: 0.2283 - accuracy: 0.9168 - val_loss: 0.4236 - val_accuracy: 0.8460 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.2163 - accuracy: 0.9221 - val_loss: 0.4226 - val_accuracy: 0.8460 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.2105 - accuracy: 0.9227 - val_loss: 0.4243 - val_accuracy: 0.8478 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.2032 - accuracy: 0.9252 - val_loss: 0.4393 - val_accuracy: 0.8482 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.1961 - accuracy: 0.9286 - val_loss: 0.4263 - val_accuracy: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.1929 - accuracy: 0.9303 - val_loss: 0.4273 - val_accuracy: 0.8498 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9322\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1856 - accuracy: 0.9322 - val_loss: 0.4229 - val_accuracy: 0.8518 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 21s 98ms/step - loss: 0.1672 - accuracy: 0.9394 - val_loss: 0.4077 - val_accuracy: 0.8568 - lr: 2.0000e-05\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.1597 - accuracy: 0.9443 - val_loss: 0.4088 - val_accuracy: 0.8566 - lr: 2.0000e-05\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1538 - accuracy: 0.9474 - val_loss: 0.4148 - val_accuracy: 0.8551 - lr: 2.0000e-05\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.1543 - accuracy: 0.9466 - val_loss: 0.4118 - val_accuracy: 0.8554 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.1544 - accuracy: 0.9454 - val_loss: 0.4139 - val_accuracy: 0.8588 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9466\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.1514 - accuracy: 0.9466 - val_loss: 0.4136 - val_accuracy: 0.8566 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.1450 - accuracy: 0.9499 - val_loss: 0.4161 - val_accuracy: 0.8558 - lr: 4.0000e-06\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1483 - accuracy: 0.9490 - val_loss: 0.4138 - val_accuracy: 0.8558 - lr: 4.0000e-06\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 0.1478 - accuracy: 0.9467 - val_loss: 0.4130 - val_accuracy: 0.8568 - lr: 4.0000e-06\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.1443 - accuracy: 0.9499 - val_loss: 0.4145 - val_accuracy: 0.8561 - lr: 4.0000e-06\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9483\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1458 - accuracy: 0.9483 - val_loss: 0.4161 - val_accuracy: 0.8555 - lr: 4.0000e-06\n",
      "Fold 3 Test Accuracy: 85.71%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 45s 123ms/step - loss: 1.2958 - accuracy: 0.3943 - val_loss: 1.2607 - val_accuracy: 0.4227 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 1.1457 - accuracy: 0.4911 - val_loss: 1.1041 - val_accuracy: 0.5119 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 1.0436 - accuracy: 0.5498 - val_loss: 1.1129 - val_accuracy: 0.5210 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.9688 - accuracy: 0.5881 - val_loss: 1.0292 - val_accuracy: 0.5590 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.9078 - accuracy: 0.6203 - val_loss: 1.0850 - val_accuracy: 0.5379 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 24s 108ms/step - loss: 0.8497 - accuracy: 0.6503 - val_loss: 1.0169 - val_accuracy: 0.5801 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.7960 - accuracy: 0.6736 - val_loss: 0.9234 - val_accuracy: 0.6284 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.7572 - accuracy: 0.6923 - val_loss: 0.8150 - val_accuracy: 0.6743 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.7139 - accuracy: 0.7116 - val_loss: 0.7372 - val_accuracy: 0.7022 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.6818 - accuracy: 0.7260 - val_loss: 0.7335 - val_accuracy: 0.7076 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 21s 94ms/step - loss: 0.6514 - accuracy: 0.7389 - val_loss: 0.7618 - val_accuracy: 0.6942 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.6200 - accuracy: 0.7538 - val_loss: 0.7059 - val_accuracy: 0.7278 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.5869 - accuracy: 0.7694 - val_loss: 0.7858 - val_accuracy: 0.6971 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.5637 - accuracy: 0.7761 - val_loss: 0.7628 - val_accuracy: 0.7114 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 24s 107ms/step - loss: 0.5433 - accuracy: 0.7840 - val_loss: 0.6773 - val_accuracy: 0.7333 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.5214 - accuracy: 0.7940 - val_loss: 0.6381 - val_accuracy: 0.7564 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.5004 - accuracy: 0.8026 - val_loss: 0.6894 - val_accuracy: 0.7345 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.4827 - accuracy: 0.8122 - val_loss: 0.7628 - val_accuracy: 0.7138 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.4661 - accuracy: 0.8190 - val_loss: 0.5650 - val_accuracy: 0.7872 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.4483 - accuracy: 0.8271 - val_loss: 0.7939 - val_accuracy: 0.7185 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.4282 - accuracy: 0.8334 - val_loss: 0.6515 - val_accuracy: 0.7566 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.4114 - accuracy: 0.8433 - val_loss: 0.5891 - val_accuracy: 0.7804 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.4041 - accuracy: 0.8437 - val_loss: 0.6535 - val_accuracy: 0.7555 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.8503\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.3908 - accuracy: 0.8503 - val_loss: 0.6346 - val_accuracy: 0.7653 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.2750 - accuracy: 0.8972 - val_loss: 0.4722 - val_accuracy: 0.8261 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.2441 - accuracy: 0.9102 - val_loss: 0.4545 - val_accuracy: 0.8387 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.2298 - accuracy: 0.9153 - val_loss: 0.4548 - val_accuracy: 0.8362 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2219 - accuracy: 0.9177 - val_loss: 0.4440 - val_accuracy: 0.8408 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2131 - accuracy: 0.9212 - val_loss: 0.4700 - val_accuracy: 0.8393 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.2087 - accuracy: 0.9237 - val_loss: 0.4700 - val_accuracy: 0.8391 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.2014 - accuracy: 0.9256 - val_loss: 0.4603 - val_accuracy: 0.8401 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.1999 - accuracy: 0.9255 - val_loss: 0.4502 - val_accuracy: 0.8471 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9286\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.1928 - accuracy: 0.9286 - val_loss: 0.4557 - val_accuracy: 0.8460 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1691 - accuracy: 0.9377 - val_loss: 0.4403 - val_accuracy: 0.8498 - lr: 2.0000e-05\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.1617 - accuracy: 0.9413 - val_loss: 0.4434 - val_accuracy: 0.8494 - lr: 2.0000e-05\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.1584 - accuracy: 0.9415 - val_loss: 0.4399 - val_accuracy: 0.8508 - lr: 2.0000e-05\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.1578 - accuracy: 0.9435 - val_loss: 0.4427 - val_accuracy: 0.8509 - lr: 2.0000e-05\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.1578 - accuracy: 0.9423 - val_loss: 0.4491 - val_accuracy: 0.8514 - lr: 2.0000e-05\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.1516 - accuracy: 0.9447 - val_loss: 0.4466 - val_accuracy: 0.8495 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.1514 - accuracy: 0.9464 - val_loss: 0.4494 - val_accuracy: 0.8534 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.9477\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.1509 - accuracy: 0.9477 - val_loss: 0.4482 - val_accuracy: 0.8511 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.1491 - accuracy: 0.9462 - val_loss: 0.4473 - val_accuracy: 0.8519 - lr: 4.0000e-06\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 24s 107ms/step - loss: 0.1479 - accuracy: 0.9480 - val_loss: 0.4442 - val_accuracy: 0.8517 - lr: 4.0000e-06\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.1449 - accuracy: 0.9485 - val_loss: 0.4432 - val_accuracy: 0.8524 - lr: 4.0000e-06\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.1423 - accuracy: 0.9500 - val_loss: 0.4440 - val_accuracy: 0.8527 - lr: 4.0000e-06\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9512\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.1401 - accuracy: 0.9512 - val_loss: 0.4472 - val_accuracy: 0.8511 - lr: 4.0000e-06\n",
      "Fold 4 Test Accuracy: 85.19%\n",
      "Epoch 1/150\n",
      "220/220 [==============================] - 46s 119ms/step - loss: 1.2996 - accuracy: 0.3924 - val_loss: 1.3003 - val_accuracy: 0.4070 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 1.1538 - accuracy: 0.4928 - val_loss: 1.1795 - val_accuracy: 0.4895 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 1.0525 - accuracy: 0.5455 - val_loss: 1.1516 - val_accuracy: 0.5054 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "220/220 [==============================] - 20s 90ms/step - loss: 0.9835 - accuracy: 0.5822 - val_loss: 0.9900 - val_accuracy: 0.5723 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "220/220 [==============================] - 19s 87ms/step - loss: 0.9201 - accuracy: 0.6147 - val_loss: 0.9616 - val_accuracy: 0.5946 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.8652 - accuracy: 0.6427 - val_loss: 0.9700 - val_accuracy: 0.6036 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "220/220 [==============================] - 21s 94ms/step - loss: 0.8161 - accuracy: 0.6635 - val_loss: 0.9473 - val_accuracy: 0.6034 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.7725 - accuracy: 0.6868 - val_loss: 0.8456 - val_accuracy: 0.6486 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.7254 - accuracy: 0.7075 - val_loss: 0.9104 - val_accuracy: 0.6313 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "220/220 [==============================] - 20s 93ms/step - loss: 0.6991 - accuracy: 0.7206 - val_loss: 0.7510 - val_accuracy: 0.6989 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.6590 - accuracy: 0.7335 - val_loss: 0.7842 - val_accuracy: 0.6797 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.6420 - accuracy: 0.7439 - val_loss: 0.6684 - val_accuracy: 0.7367 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.6039 - accuracy: 0.7612 - val_loss: 0.6857 - val_accuracy: 0.7302 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "220/220 [==============================] - 23s 103ms/step - loss: 0.5762 - accuracy: 0.7702 - val_loss: 0.6788 - val_accuracy: 0.7289 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.5511 - accuracy: 0.7803 - val_loss: 0.6349 - val_accuracy: 0.7541 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.5389 - accuracy: 0.7878 - val_loss: 0.6609 - val_accuracy: 0.7434 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.5151 - accuracy: 0.7948 - val_loss: 0.6225 - val_accuracy: 0.7579 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.4975 - accuracy: 0.8054 - val_loss: 0.5681 - val_accuracy: 0.7781 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "220/220 [==============================] - 22s 100ms/step - loss: 0.4714 - accuracy: 0.8172 - val_loss: 0.6243 - val_accuracy: 0.7593 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.4617 - accuracy: 0.8205 - val_loss: 0.5942 - val_accuracy: 0.7701 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.4449 - accuracy: 0.8274 - val_loss: 0.5797 - val_accuracy: 0.7778 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "220/220 [==============================] - 23s 104ms/step - loss: 0.4271 - accuracy: 0.8354 - val_loss: 0.5744 - val_accuracy: 0.7804 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.8359\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.4215 - accuracy: 0.8359 - val_loss: 0.6565 - val_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2977 - accuracy: 0.8912 - val_loss: 0.4337 - val_accuracy: 0.8404 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.2685 - accuracy: 0.8988 - val_loss: 0.4467 - val_accuracy: 0.8364 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.2572 - accuracy: 0.9039 - val_loss: 0.4471 - val_accuracy: 0.8370 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2446 - accuracy: 0.9105 - val_loss: 0.4209 - val_accuracy: 0.8482 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "220/220 [==============================] - 23s 106ms/step - loss: 0.2362 - accuracy: 0.9121 - val_loss: 0.4326 - val_accuracy: 0.8444 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2297 - accuracy: 0.9132 - val_loss: 0.4331 - val_accuracy: 0.8485 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "220/220 [==============================] - 23s 105ms/step - loss: 0.2236 - accuracy: 0.9162 - val_loss: 0.4370 - val_accuracy: 0.8492 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "220/220 [==============================] - 22s 102ms/step - loss: 0.2167 - accuracy: 0.9213 - val_loss: 0.4338 - val_accuracy: 0.8447 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9186\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.2161 - accuracy: 0.9186 - val_loss: 0.4357 - val_accuracy: 0.8461 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1886 - accuracy: 0.9302 - val_loss: 0.4203 - val_accuracy: 0.8560 - lr: 2.0000e-05\n",
      "Epoch 34/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1819 - accuracy: 0.9341 - val_loss: 0.4142 - val_accuracy: 0.8575 - lr: 2.0000e-05\n",
      "Epoch 35/150\n",
      "220/220 [==============================] - 22s 98ms/step - loss: 0.1784 - accuracy: 0.9370 - val_loss: 0.4196 - val_accuracy: 0.8565 - lr: 2.0000e-05\n",
      "Epoch 36/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.1764 - accuracy: 0.9365 - val_loss: 0.4084 - val_accuracy: 0.8600 - lr: 2.0000e-05\n",
      "Epoch 37/150\n",
      "220/220 [==============================] - 21s 96ms/step - loss: 0.1763 - accuracy: 0.9360 - val_loss: 0.4118 - val_accuracy: 0.8600 - lr: 2.0000e-05\n",
      "Epoch 38/150\n",
      "220/220 [==============================] - 21s 95ms/step - loss: 0.1758 - accuracy: 0.9348 - val_loss: 0.4178 - val_accuracy: 0.8586 - lr: 2.0000e-05\n",
      "Epoch 39/150\n",
      "220/220 [==============================] - 21s 94ms/step - loss: 0.1735 - accuracy: 0.9370 - val_loss: 0.4113 - val_accuracy: 0.8606 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "220/220 [==============================] - 20s 93ms/step - loss: 0.1716 - accuracy: 0.9384 - val_loss: 0.4107 - val_accuracy: 0.8606 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9389\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1660 - accuracy: 0.9389 - val_loss: 0.4124 - val_accuracy: 0.8587 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "220/220 [==============================] - 21s 94ms/step - loss: 0.1643 - accuracy: 0.9414 - val_loss: 0.4142 - val_accuracy: 0.8596 - lr: 4.0000e-06\n",
      "Epoch 43/150\n",
      "220/220 [==============================] - 21s 97ms/step - loss: 0.1631 - accuracy: 0.9423 - val_loss: 0.4146 - val_accuracy: 0.8599 - lr: 4.0000e-06\n",
      "Epoch 44/150\n",
      "220/220 [==============================] - 22s 101ms/step - loss: 0.1613 - accuracy: 0.9423 - val_loss: 0.4089 - val_accuracy: 0.8613 - lr: 4.0000e-06\n",
      "Epoch 45/150\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.1600 - accuracy: 0.9429 - val_loss: 0.4146 - val_accuracy: 0.8597 - lr: 4.0000e-06\n",
      "Epoch 46/150\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9418\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "220/220 [==============================] - 22s 99ms/step - loss: 0.1616 - accuracy: 0.9418 - val_loss: 0.4113 - val_accuracy: 0.8627 - lr: 4.0000e-06\n",
      "Fold 5 Test Accuracy: 85.38%\n",
      "all_Vali: [85.43741106987, 85.74272990226746, 85.70954203605652, 85.19182205200195, 85.37766933441162]\n",
      "Mean acc: 85.49183487892151\n",
      "Std acc: 0.20801352738163414\n",
      "Run time: 5291.17 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Concatenate, \n",
    "                          Reshape, BatchNormalization, Bidirectional, GRU, Add, MultiHeadAttention, \n",
    "                          LayerNormalization, GlobalAveragePooling1D)\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Data Loading and Reshaping\n",
    "num_classes = 4\n",
    "batch_size = 128\n",
    "img_rows, img_cols, num_chan = 8, 9, 4\n",
    "\n",
    "falx = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t6x_89.npy\")\n",
    "y = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session_1_2_3/t6y_89.npy\")\n",
    "print('{}-{}'.format('falx shape', falx.shape))\n",
    "print('{}-{}'.format('y shape', y.shape))\n",
    "\n",
    "one_y = to_categorical(y, num_classes)\n",
    "print('{}-{}'.format('one_y categorical shape', one_y.shape))\n",
    "\n",
    "one_falx_1 = falx.reshape((-1, 6, img_rows, img_cols, 5))  # reshape each person's segments\n",
    "one_falx = one_falx_1[:, :, :, :, 1:5]  # only 4 bands\n",
    "\n",
    "#=========================== Fixed 30% Test Set Before K-Fold ===========================\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(one_falx, one_y,\n",
    "                                                            test_size=0.3,\n",
    "                                                            random_state=42,\n",
    "                                                            stratify=one_y.argmax(1))\n",
    "print(\"X_train_all shape:\", X_train_all.shape)\n",
    "print(\"y_train_all shape:\", y_train_all.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Base CNN Network with Residual Blocks\n",
    "def create_base_network(input_dim):\n",
    "    input_layer = Input(shape=input_dim)\n",
    "    x = Conv2D(32, 5, activation='relu', padding='same', name='conv1')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # First Residual Block\n",
    "    residual = Conv2D(128, 4, activation='relu', padding='same', name='conv2')(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "    residual = Dropout(0.2)(residual)\n",
    "    x = Conv2D(128, 4, activation='relu', padding='same', name='conv2_residual')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Add()([x, residual])\n",
    "    \n",
    "    # Second Residual Block\n",
    "    residual = Conv2D(256, 4, activation='relu', padding='same', name='conv3')(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "    residual = Dropout(0.2)(residual)\n",
    "    x = Conv2D(256, 4, activation='relu', padding='same', name='conv3_residual')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Add()([x, residual])\n",
    "    \n",
    "    x = Conv2D(64, 1, activation='relu', padding='same', name='conv4')(x)\n",
    "    x = MaxPooling2D(2, 2, name='pool1')(x)\n",
    "    x = Flatten(name='fla1')(x)\n",
    "    x = Dense(256, activation='relu', name='dense1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Reshape((1, 256), name='reshape')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Vision Transformer Encoder Block\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    # Multi-Head Self Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    # Add & Norm\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    # Feed Forward Network\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "acc_list = []\n",
    "std_list = []\n",
    "all_acc = []\n",
    "\n",
    "K.clear_session()\n",
    "start = time.time()\n",
    "\n",
    "img_size = (img_rows, img_cols, num_chan)\n",
    "\n",
    "# K-Fold on the 70% training data\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train_all)):\n",
    "    X_fold_train, X_fold_val = X_train_all[train_indices], X_train_all[val_indices]\n",
    "    y_fold_train, y_fold_val = y_train_all[train_indices], y_train_all[val_indices]\n",
    "\n",
    "    # Create base network\n",
    "    base_network = create_base_network(img_size)\n",
    "    \n",
    "    inputs = [Input(shape=img_size) for _ in range(6)]\n",
    "    out_all = Concatenate(axis=1)([base_network(inp) for inp in inputs])\n",
    "\n",
    "    # Transformer Encoder Block\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    transformer_output = transformer_encoder(out_all, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "    # Bidirectional GRU Layer\n",
    "    bidir_gru = Bidirectional(GRU(128, return_sequences=False))(transformer_output)\n",
    "\n",
    "    # Output Layer\n",
    "    out_layer = Dense(4, activation='softmax', name='out')(bidir_gru)\n",
    "    model = Model(inputs, out_layer)\n",
    "\n",
    "    # TensorBoard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    # Train the model on this fold's train/val splits\n",
    "    model.fit(\n",
    "        [X_fold_train[:, i, :, :, :] for i in range(6)],\n",
    "        y_fold_train,\n",
    "        epochs=150,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        validation_data=([X_fold_val[:, i, :, :, :] for i in range(6)], y_fold_val),\n",
    "        callbacks=[early_stopping, reduce_lr, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the fixed 30% test set\n",
    "    scores = model.evaluate(\n",
    "        [X_test[:, i, :, :, :] for i in range(6)],\n",
    "        y_test,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.save(f'ResCNN-TransGRU_fold{fold+1}.h5')\n",
    "    \n",
    "    print(f\"Fold {fold+1} Test Accuracy: {scores[1] * 100:.2f}%\")\n",
    "    all_acc.append(scores[1] * 100)\n",
    "\n",
    "print(\"all_Vali:\", all_acc)\n",
    "print(\"Mean acc:\", np.mean(all_acc))\n",
    "print(\"Std acc:\", np.std(all_acc))\n",
    "end = time.time()\n",
    "print(\"Run time: %.2f seconds\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadeb306-da1b-4054-9f40-10409771e81c",
   "metadata": {},
   "source": [
    "### ouput\n",
    "- all_Vali: [85.43741106987, 85.74272990226746, 85.70954203605652, 85.19182205200195, 85.37766933441162]\n",
    "- Mean acc: 85.49183487892151\n",
    "- Std acc: 0.20801352738163414"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
