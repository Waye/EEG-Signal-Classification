{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6289c1d-a69f-4cbf-b51a-b25debebbf83",
   "metadata": {},
   "source": [
    "# Data Preprocssing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5363f-1354-4c39-9aff-26f498ce0e04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Outlier Removal Before Filtering\n",
    "Why: Removing outliers before bandpass filtering prevents filter artifacts and ensures the filtering process operates on clean data.\n",
    "Method: The remove_outliers function detects outliers using Z-score thresholding and replaces them with interpolated values.\n",
    "\n",
    "## Zero-Phase Filtering\n",
    "Implementation: Replaced lfilter with filtfilt in the butter_bandpass_filter function.\n",
    "Benefit: Zero-phase filtering avoids phase distortions that can be introduced by standard filtering methods, preserving the temporal characteristics of the EEG signals.\n",
    "\n",
    "## Differential Entropy Computation\n",
    "Adjustment: Added a small constant (1e-6) to the variance in compute_DE to prevent mathematical errors due to zero variance.\n",
    "Importance: Ensures that DE values are computed accurately without encountering logarithm of zero.\n",
    "\n",
    "## Data Reshaping to 10-20 System\n",
    "Purpose: Aligns the processed data with the standard 10-20 EEG electrode placement system.\n",
    "Manual Mapping: Channels are manually assigned to positions on an 8x9 grid. This requires careful attention to ensure accurate representation.\n",
    "\n",
    "## Data Segregation\n",
    "Trial Groups: Data and labels are divided into two groups (trials 1-16 and 17-24) for separate analysis or validation.\n",
    "Consistency: Ensures that labels are correctly assigned to each segment based on the trial and participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ed4340-ba72-4ad6-bb8f-08305fc55f19",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1_20160518\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 1_20161125\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 1_20161126\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 2_20150915\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 2_20150920\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 2_20151012\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 3_20150919\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 3_20151018\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 3_20151101\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 4_20151111\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 4_20151118\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 4_20151123\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 5_20160406\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 5_20160413\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 5_20160420\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 6_20150507\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 6_20150511\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 6_20150512\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 7_20150715\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 7_20150717\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 7_20150721\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 8_20151103\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 8_20151110\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 8_20151117\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 9_20151028\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 9_20151119\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 9_20151209\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 10_20151014\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 10_20151021\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 10_20151023\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 11_20150916\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 11_20150921\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 11_20151011\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 12_20150725\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 12_20150804\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 12_20150807\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 13_20151115\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 13_20151125\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 13_20161130\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 14_20151205\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 14_20151208\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 14_20151215\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n",
      "processing 15_20150508\n",
      "1-336\n",
      "2-190\n",
      "3-398\n",
      "4-260\n",
      "5-176\n",
      "6-324\n",
      "7-306\n",
      "8-418\n",
      "9-290\n",
      "10-338\n",
      "11-100\n",
      "12-220\n",
      "13-434\n",
      "14-338\n",
      "15-518\n",
      "16-282\n",
      "17-136\n",
      "18-358\n",
      "19-280\n",
      "20-96\n",
      "21-224\n",
      "22-224\n",
      "23-350\n",
      "24-274\n",
      "processing 15_20150514\n",
      "1-442\n",
      "2-202\n",
      "3-278\n",
      "4-292\n",
      "5-428\n",
      "6-222\n",
      "7-278\n",
      "8-368\n",
      "9-276\n",
      "10-166\n",
      "11-480\n",
      "12-100\n",
      "13-292\n",
      "14-216\n",
      "15-352\n",
      "16-122\n",
      "17-374\n",
      "18-392\n",
      "19-364\n",
      "20-86\n",
      "21-298\n",
      "22-352\n",
      "23-196\n",
      "24-152\n",
      "processing 15_20150527\n",
      "1-340\n",
      "2-260\n",
      "3-184\n",
      "4-364\n",
      "5-386\n",
      "6-212\n",
      "7-516\n",
      "8-186\n",
      "9-208\n",
      "10-128\n",
      "11-414\n",
      "12-330\n",
      "13-314\n",
      "14-154\n",
      "15-230\n",
      "16-354\n",
      "17-114\n",
      "18-140\n",
      "19-366\n",
      "20-178\n",
      "21-318\n",
      "22-310\n",
      "23-330\n",
      "24-312\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, filtfilt  # Changed lfilter to filtfilt for zero-phase filtering\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Define label sets for different types of mat files\n",
    "label_sets = {\n",
    "    \"type_1\": [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3],\n",
    "    \"type_2\": [2, 1, 3, 0, 0, 2, 0, 2, 3, 3, 2, 3, 2, 0, 1, 1, 2, 1, 0, 3, 0, 1, 3, 1],\n",
    "    \"type_3\": [1, 2, 2, 1, 3, 3, 3, 1, 1, 2, 1, 0, 2, 3, 3, 0, 2, 3, 0, 0, 2, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Function to remove outliers from a signal\n",
    "def remove_outliers(signal, method='zscore', threshold=3):\n",
    "    if method == 'zscore':\n",
    "        mean = np.mean(signal)\n",
    "        std = np.std(signal)\n",
    "        z_scores = np.abs((signal - mean) / std)\n",
    "        mask = z_scores < threshold\n",
    "        signal_clean = signal.copy()\n",
    "        # Replace outliers with interpolated values\n",
    "        indices = np.arange(len(signal))\n",
    "        signal_clean[~mask] = np.interp(indices[~mask], indices[mask], signal_clean[mask])\n",
    "    elif method == 'mad':\n",
    "        median = np.median(signal)\n",
    "        mad = np.median(np.abs(signal - median))\n",
    "        mask = np.abs(signal - median) < threshold * mad\n",
    "        signal_clean = signal.copy()\n",
    "        indices = np.arange(len(signal))\n",
    "        signal_clean[~mask] = np.interp(indices[~mask], indices[mask], signal_clean[mask])\n",
    "    else:\n",
    "        raise ValueError(\"Method not recognized.\")\n",
    "    return signal_clean\n",
    "\n",
    "# Function to create bandpass filter coefficients\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs  # Nyquist frequency\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "# Function to apply bandpass filter to data using zero-phase filtering\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)  # Use filtfilt for zero-phase filtering\n",
    "    return y\n",
    "\n",
    "# Function to compute Differential Entropy (DE) of a signal\n",
    "def compute_DE(signal):\n",
    "    variance = np.var(signal, ddof=1) + 1e-6  # Add small constant to prevent log(0)\n",
    "    return 0.5 * np.log(2 * np.pi * np.e * variance)  # Return DE value\n",
    "\n",
    "# Function to decompose EEG data into different frequency bands and extract DE features\n",
    "def decompose(file, name, label_set):\n",
    "    # Load the .mat file containing the EEG data\n",
    "    data = loadmat(file)\n",
    "    frequency = 200  # Sampling rate of the SEED dataset is downsampled to 200Hz\n",
    "\n",
    "    # Create empty arrays to store DE features and labels for both trial sets\n",
    "    decomposed_de_1_16 = np.empty([0, 62, 5])\n",
    "    decomposed_de_17_24 = np.empty([0, 62, 5])\n",
    "    label_1_16 = np.array([])\n",
    "    label_17_24 = np.array([])\n",
    "\n",
    "    # Loop through all 24 trials in the dataset\n",
    "    for trial in range(24):\n",
    "        # Load the EEG data for the current trial\n",
    "        tmp_trial_signal = data[name + '_eeg' + str(trial + 1)]\n",
    "        # Number of samples per segment (0.5 seconds per segment, with a sampling rate of 200Hz)\n",
    "        num_sample = int(len(tmp_trial_signal[0]) / 100)\n",
    "        print('{}-{}'.format(trial + 1, num_sample))\n",
    "\n",
    "        # Initialize temporary array to store DE features for each channel\n",
    "        temp_de = np.empty([0, num_sample])\n",
    "        # Assign labels for each sample in the current trial\n",
    "        if trial < 16:\n",
    "            label_1_16 = np.append(label_1_16, [label_set[trial]] * num_sample)\n",
    "        else:\n",
    "            label_17_24 = np.append(label_17_24, [label_set[trial]] * num_sample)\n",
    "\n",
    "        # Loop through each channel (total 62 channels)\n",
    "        for channel in range(62):\n",
    "            trial_signal = tmp_trial_signal[channel]\n",
    "\n",
    "            # Remove outliers before filtering\n",
    "            trial_signal = remove_outliers(trial_signal, method='zscore', threshold=3)\n",
    "\n",
    "            # Apply bandpass filters to extract different frequency bands\n",
    "            delta = butter_bandpass_filter(trial_signal, 1, 4, frequency, order=3)\n",
    "            theta = butter_bandpass_filter(trial_signal, 4, 8, frequency, order=3)\n",
    "            alpha = butter_bandpass_filter(trial_signal, 8, 14, frequency, order=3)\n",
    "            beta = butter_bandpass_filter(trial_signal, 14, 31, frequency, order=3)\n",
    "            gamma = butter_bandpass_filter(trial_signal, 31, 51, frequency, order=3)\n",
    "\n",
    "            # Initialize arrays to store DE values for each frequency band\n",
    "            DE_delta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_theta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_alpha = np.zeros(shape=[0], dtype=float)\n",
    "            DE_beta = np.zeros(shape=[0], dtype=float)\n",
    "            DE_gamma = np.zeros(shape=[0], dtype=float)\n",
    "\n",
    "            # Compute DE features for each frequency band in each segment\n",
    "            for index in range(num_sample):\n",
    "                start_idx = index * 100\n",
    "                end_idx = (index + 1) * 100\n",
    "                DE_delta = np.append(DE_delta, compute_DE(delta[start_idx:end_idx]))\n",
    "                DE_theta = np.append(DE_theta, compute_DE(theta[start_idx:end_idx]))\n",
    "                DE_alpha = np.append(DE_alpha, compute_DE(alpha[start_idx:end_idx]))\n",
    "                DE_beta = np.append(DE_beta, compute_DE(beta[start_idx:end_idx]))\n",
    "                DE_gamma = np.append(DE_gamma, compute_DE(gamma[start_idx:end_idx]))\n",
    "\n",
    "            # Stack the DE features for each frequency band\n",
    "            temp_de = np.vstack([temp_de, DE_delta])\n",
    "            temp_de = np.vstack([temp_de, DE_theta])\n",
    "            temp_de = np.vstack([temp_de, DE_alpha])\n",
    "            temp_de = np.vstack([temp_de, DE_beta])\n",
    "            temp_de = np.vstack([temp_de, DE_gamma])\n",
    "\n",
    "        # Reshape the DE features to match the desired format\n",
    "        temp_trial_de = temp_de.reshape(-1, 5, num_sample)\n",
    "        temp_trial_de = temp_trial_de.transpose([2, 0, 1])  # Rearrange dimensions to match desired format\n",
    "\n",
    "        # Stack trial data into the appropriate group\n",
    "        if trial < 16:\n",
    "            decomposed_de_1_16 = np.vstack([decomposed_de_1_16, temp_trial_de])\n",
    "        else:\n",
    "            decomposed_de_17_24 = np.vstack([decomposed_de_17_24, temp_trial_de])\n",
    "\n",
    "    return decomposed_de_1_16, label_1_16, decomposed_de_17_24, label_17_24\n",
    "\n",
    "# Main script to extract features and save data\n",
    "file_path = 'D:/BigData/SEED_IV/SEED_IV/eeg_raw_data/'\n",
    "\n",
    "# List of participant names and short names used in file naming\n",
    "people_name = ['1_20160518', '1_20161125', '1_20161126',\n",
    "               '2_20150915', '2_20150920', '2_20151012',\n",
    "               '3_20150919', '3_20151018', '3_20151101',\n",
    "               '4_20151111', '4_20151118', '4_20151123',\n",
    "               '5_20160406', '5_20160413', '5_20160420',\n",
    "               '6_20150507', '6_20150511', '6_20150512',\n",
    "               '7_20150715', '7_20150717', '7_20150721',\n",
    "               '8_20151103', '8_20151110', '8_20151117',\n",
    "               '9_20151028', '9_20151119', '9_20151209',\n",
    "               '10_20151014', '10_20151021', '10_20151023',\n",
    "               '11_20150916', '11_20150921', '11_20151011',\n",
    "               '12_20150725', '12_20150804', '12_20150807',\n",
    "               '13_20151115', '13_20151125', '13_20161130',\n",
    "               '14_20151205', '14_20151208', '14_20151215',\n",
    "               '15_20150508', '15_20150514', '15_20150527']\n",
    "\n",
    "short_name = ['cz', 'cz', 'cz',\n",
    "              'ha', 'ha', 'ha',\n",
    "              'hql', 'hql', 'hql',\n",
    "              'ldy', 'ldy', 'ldy',\n",
    "              'ly', 'ly', 'ly',\n",
    "              'mhw', 'mhw', 'mhw',\n",
    "              'mz', 'mz', 'mz',\n",
    "              'qyt', 'qyt', 'qyt',\n",
    "              'rx', 'rx', 'rx',\n",
    "              'tyc', 'tyc', 'tyc',\n",
    "              'whh', 'whh', 'whh',\n",
    "              'wll', 'wll', 'wll',\n",
    "              'wq', 'wq', 'wq',\n",
    "              'zjd', 'zjd', 'zjd',\n",
    "              'zjy', 'zjy', 'zjy']\n",
    "\n",
    "# Initialize empty arrays for storing the final DE features and labels for both trial groups\n",
    "X_1_16 = np.empty([0, 62, 5])\n",
    "y_1_16 = np.empty([0])\n",
    "X_17_24 = np.empty([0, 62, 5])\n",
    "y_17_24 = np.empty([0])\n",
    "\n",
    "# Loop through all participants to extract DE features\n",
    "for i in range(len(people_name)):  # Loop through all 45 experiments (15 participants, 3 trials each)\n",
    "    file_name = os.path.join(file_path, people_name[i])\n",
    "    print('processing {}'.format(people_name[i]))\n",
    "\n",
    "    # Select the appropriate label set based on the type of file\n",
    "    if i % 3 == 0:\n",
    "        label_set = label_sets[\"type_1\"]\n",
    "    elif i % 3 == 1:\n",
    "        label_set = label_sets[\"type_2\"]\n",
    "    else:\n",
    "        label_set = label_sets[\"type_3\"]\n",
    "\n",
    "    decomposed_de_1_16, label_1_16, decomposed_de_17_24, label_17_24 = decompose(file_name, short_name[i], label_set)\n",
    "\n",
    "    # Stack the features and labels for the two trial groups\n",
    "    X_1_16 = np.vstack([X_1_16, decomposed_de_1_16])\n",
    "    y_1_16 = np.append(y_1_16, label_1_16)\n",
    "    X_17_24 = np.vstack([X_17_24, decomposed_de_17_24])\n",
    "    y_17_24 = np.append(y_17_24, label_17_24)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the extracted DE features and labels as .npy files\n",
    "np.save(os.path.join(output_dir, \"X_1_16.npy\"), X_1_16)\n",
    "np.save(os.path.join(output_dir, \"y_1_16.npy\"), y_1_16)\n",
    "np.save(os.path.join(output_dir, \"X_17_24.npy\"), X_17_24)\n",
    "np.save(os.path.join(output_dir, \"y_17_24.npy\"), y_17_24)\n",
    "\n",
    "# Load the saved features and labels\n",
    "X_1_16 = np.load(os.path.join(output_dir, 'X_1_16.npy'))\n",
    "y_1_16 = np.load(os.path.join(output_dir, 'y_1_16.npy'))\n",
    "X_17_24 = np.load(os.path.join(output_dir, 'X_17_24.npy'))\n",
    "y_17_24 = np.load(os.path.join(output_dir, 'y_17_24.npy'))\n",
    "\n",
    "def reshape(y, X):\n",
    "    # Reshape the 62-channel data into an 8x9 matrix (based on the 10-20 electrode system)\n",
    "    X89 = np.zeros((len(y), 8, 9, 5))  # Create an empty array for the 8x9x5 data\n",
    "    X89[:, 0, 2, :] = X[:, 3, :]  # Assign values to specific positions in the 8x9 matrix\n",
    "    X89[:, 0, 3:6, :] = X[:, 0:3, :]\n",
    "    X89[:, 0, 6, :] = X[:, 4, :]\n",
    "\n",
    "    # Assign values for the middle rows of the 8x9 matrix\n",
    "    for i in range(5):\n",
    "        X89[:, i + 1, :, :] = X[:, 5 + i * 9:5 + (i + 1) * 9, :]\n",
    "\n",
    "    # Assign values for the last two rows of the 8x9 matrix\n",
    "    X89[:, 6, 1:8, :] = X[:, 50:57, :]\n",
    "    X89[:, 7, 2:7, :] = X[:, 57:62, :]\n",
    "    return X89\n",
    "\n",
    "X89_1_16 = reshape(y_1_16, X_1_16)\n",
    "X89_17_24 = reshape(y_17_24, X_17_24)\n",
    "\n",
    "# Save the reshaped 8x9 matrix data\n",
    "np.save(os.path.join(output_dir, \"X89_1_16.npy\"), X89_1_16)\n",
    "np.save(os.path.join(output_dir, \"X89_17_24.npy\"), X89_17_24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2311f-86de-4692-ac84-346f2faf3e38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading and Reshaping Data\n",
    "Purpose: Load preprocessed EEG data and reshape it to group data by participants.\n",
    "Implementation: Loaded data from .npy files (X89_1_16.npy, X89_17_24.npy) and reshaped them into arrays of shape (15, num_samples_per_participant, 8, 9, 5) to organize the data by the 15 participants.\n",
    "\n",
    "## Defining Segment Length\n",
    "Segment Length (t): Set to 6, which corresponds to a 3-second segment (since each time step represents 0.5 seconds).\n",
    "Purpose: Capture temporal dependencies over meaningful durations in the EEG data for time-series analysis.\n",
    "\n",
    "## Segmenting Data into Fixed-Length Sequences\n",
    "Function: segment_data(falx, t, lengths, labels)\n",
    "Purpose: Segment the data into fixed-length sequences and assign corresponding labels.\n",
    "Method:\n",
    "- Calculated trial boundaries using cumulative sums of the trial lengths provided in lengths.\n",
    "- Iterated over participants (nb) and trials (j), extracting non-overlapping segments of length t.\n",
    "- Managed indices carefully to ensure correct segment extraction without overlap.\n",
    "- Assigned labels to each segment based on the trial's label from the labels list.\n",
    "- Resized the pre-allocated array new_x to exclude unused space after segmentation.\n",
    "\n",
    "## Defining Trial Lengths and Labels\n",
    "Purpose: Provide necessary information for accurate segmentation and label assignment.\n",
    "Data:\n",
    "lengths_1_16 and lengths_17_24: Lists containing the number of time steps for each trial in datasets 1-16 and 17-24, respectively.\n",
    "all_label_1_16 and all_label_17_24: Lists containing labels corresponding to each trial in the respective datasets.\n",
    "\n",
    "## Segmenting and Saving Data\n",
    "Process:\n",
    "- Applied the segment_data function to both datasets:\n",
    "  - For trials 1-16: x_89_Seg_1_16, y_89_Seg_1_16 were obtained from falx_1_16.\n",
    "  - For trials 17-24: x_89_Seg_17_24, y_89_Seg_17_24 were obtained from falx_17_24.\n",
    "- Printed shapes of the resulting segmented data and labels to verify correctness.\n",
    "\n",
    "Saving:\n",
    "- Saved the segmented data and labels to .npy files:\n",
    "  - Segmented data: t6x_89_Seg_1_16.npy, t6x_89_Seg_17_24.npy.\n",
    "  - Labels: t6y_89_Seg_1_16.npy, t6y_89_Seg_17_24.npy.\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a34c78-4518-4447-93bf-90f0613a332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape-(210330, 8, 9, 5)\n",
      "falx_1_16 shape-(15, 14022, 8, 9, 5)\n",
      "x_train shape-(93360, 8, 9, 5)\n",
      "falx_17_24 shape-(15, 6224, 8, 9, 5)\n",
      "boundaries-[  336   526   924  1184  1360  1684  1990  2408  2698  3036  3136  3356\n",
      "  3790  4128  4646  4928  5370  5572  5850  6142  6570  6792  7070  7438\n",
      "  7714  7880  8360  8460  8752  8968  9320  9442  9782 10042 10226 10590\n",
      " 10976 11188 11704 11890 12098 12226 12640 12970 13284 13438 13668 14022]\n",
      "total_segments-100000\n",
      "0-2320\n",
      "1-2320\n",
      "2-2320\n",
      "3-2320\n",
      "4-2320\n",
      "5-2320\n",
      "6-2320\n",
      "7-2320\n",
      "8-2320\n",
      "9-2320\n",
      "10-2320\n",
      "11-2320\n",
      "12-2320\n",
      "13-2320\n",
      "14-2320\n",
      "boundaries-[ 136  494  774  870 1094 1318 1668 1942 2316 2708 3072 3158 3456 3808\n",
      " 4004 4156 4270 4410 4776 4954 5272 5582 5912 6224]\n",
      "total_segments-100000\n",
      "0-1028\n",
      "1-1028\n",
      "2-1028\n",
      "3-1028\n",
      "4-1028\n",
      "5-1028\n",
      "6-1028\n",
      "7-1028\n",
      "8-1028\n",
      "9-1028\n",
      "10-1028\n",
      "11-1028\n",
      "12-1028\n",
      "13-1028\n",
      "14-1028\n",
      "x_89_Seg_1_16 shape-(15, 2320, 6, 8, 9, 5)\n",
      "y_89_Seg_1_16 shape-(34800,)\n",
      "x_89_Seg_17_24 shape-(15, 1028, 6, 8, 9, 5)\n",
      "y_89_Seg_17_24 shape-(15420,)\n"
     ]
    }
   ],
   "source": [
    "# segements\n",
    "import numpy as np\n",
    "\n",
    "X89_1_16 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/X89_1_16.npy\")\n",
    "print('{}-{}'.format('x_train shape', X89_1_16.shape))#x_train shape-(210330, 8, 9, 5)\n",
    "img_rows, img_cols, num_chan = 8, 9, 5\n",
    "falx_1_16 = X89_1_16\n",
    "falx_1_16 = falx_1_16.reshape((15, int(X89_1_16.shape[0] / 15), img_rows, img_cols, num_chan)) #falx_1_16 shape-(15, 14022, 8, 9, 5)\n",
    "print('{}-{}'.format('falx_1_16 shape', falx_1_16.shape))\n",
    "\n",
    "X89_17_24 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/X89_17_24.npy\")\n",
    "print('{}-{}'.format('x_train shape', X89_17_24.shape))#x_train shape-(93360, 8, 9, 5)\n",
    "img_rows, img_cols, num_chan = 8, 9, 5\n",
    "falx_17_24 = X89_17_24\n",
    "falx_17_24 = falx_17_24.reshape((15, int(X89_17_24.shape[0] / 15), img_rows, img_cols, num_chan)) #falx_17_24 shape-(15, 6224, 8, 9, 5)\n",
    "print('{}-{}'.format('falx_17_24 shape', falx_17_24.shape))\n",
    "\n",
    "t = 6 #(0.5s ->3s segement  6 pieces)\n",
    "\n",
    "def segment_data(falx, t, lengths, labels):\n",
    "  \"\"\"Segments data into fixed-length segments with corresponding labels.\n",
    "\n",
    "  Args:\n",
    "    falx: The input data array.\n",
    "    t: The length of each segment.\n",
    "    lengths: A list of lengths for each segment.\n",
    "    labels: A list of labels corresponding to each segment.\n",
    "\n",
    "  Returns:\n",
    "    new_x: The segmented data array.\n",
    "    new_y: The corresponding label array.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate boundaries from lengths\n",
    "  boundaries = np.cumsum(lengths)\n",
    "  print('{}-{}'.format('boundaries', boundaries))\n",
    "\n",
    "  total_segments = 100000 #pre-allocated number\n",
    "  print('{}-{}'.format('total_segments', total_segments))\n",
    "\n",
    "  # Pre-allocate new_x with correct dimensions\n",
    "  new_x = np.empty([falx.shape[0], total_segments, t, 8, 9, 5])  \n",
    "  new_y = np.array([])\n",
    "\n",
    "  for nb in range(falx.shape[0]):\n",
    "    z = 0\n",
    "    i = 0\n",
    "    for j, bound in enumerate(boundaries):\n",
    "      while i + t <= bound:\n",
    "        # Assign segments directly, taking all 6 time steps at once\n",
    "        new_x[nb, z] = falx[nb, i:i + t]  # Assign to the first t indices of the segment\n",
    "        new_y = np.append(new_y, labels[j])\n",
    "        i = i + t\n",
    "        z = z + 1\n",
    "      i = bound\n",
    "    print('{}-{}'.format(nb, z))\n",
    "    # Resize new_x to exclude empty cells (segments that were not filled)\n",
    "    new_x = new_x[:, :z, :, :, :, :]\n",
    "  return new_x, new_y\n",
    "\n",
    "lengths_1_16 =[336,190,398,260,176,324,306,418,290,338,100,220,434,338,518,282,\n",
    "          442, 202, 278,292,428,222,278, 368, 276, 166, 480, 100, 292, 216, 352, 122, \n",
    "          340, 260, 184, 364, 386, 212, 516, 186, 208, 128, 414, 330, 314, 154, 230, 354]\n",
    "lengths_17_24 =[136,358,280,96,224,224,350,274, \n",
    "          374, 392, 364, 86, 298, 352, 196, 152,\n",
    "          114, 140, 366, 178, 318, 310, 330, 312]\n",
    "all_label_1_16 = [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,\n",
    "             2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,\n",
    "             1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0]\n",
    "all_label_17_24 = [2,2,3,3,0,3,0,3,\n",
    "                   2,1,0,3,0,1,3,1,\n",
    "                   2,3,0,0,2,0,1,0]\n",
    "\n",
    "x_89_Seg_1_16, y_89_Seg_1_16 = segment_data(falx_1_16, 6, lengths_1_16, all_label_1_16)\n",
    "x_89_Seg_17_24, y_89_Seg_17_24 = segment_data(falx_17_24, 6, lengths_17_24, all_label_17_24)\n",
    "\n",
    "print('{}-{}'.format('x_89_Seg_1_16 shape', x_89_Seg_1_16.shape))\n",
    "print('{}-{}'.format('y_89_Seg_1_16 shape', y_89_Seg_1_16.shape))\n",
    "print('{}-{}'.format('x_89_Seg_17_24 shape', x_89_Seg_17_24.shape))\n",
    "print('{}-{}'.format('y_89_Seg_17_24 shape', y_89_Seg_17_24.shape))\n",
    "\n",
    "np.save('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t'+str(t)+'x_89_Seg_1_16.npy', x_89_Seg_1_16)#(15, 2320, 6, 8, 9, 5)\n",
    "np.save('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t'+str(t)+'y_89_Seg_1_16.npy', y_89_Seg_1_16)#(34800,)\n",
    "np.save('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t'+str(t)+'x_89_Seg_17_24.npy', x_89_Seg_17_24)#(15, 1028, 6, 8, 9, 5)\n",
    "np.save('D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t'+str(t)+'y_89_Seg_17_24.npy', y_89_Seg_17_24)#(15420,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06501e06-677e-4209-a8a6-d80acf2d68a8",
   "metadata": {},
   "source": [
    "# Model Traning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5782467-fbb3-4a4e-abe6-8195569a6dfe",
   "metadata": {},
   "source": [
    "## Objective: Train a deep learning model for emotion recognition from EEG data using a combination of CNN, Transformer, and GRU layers.\n",
    "\n",
    "## Data Handling:\n",
    "\n",
    "Loaded and normalized EEG data.\n",
    "Prepared data for input into the model by reshaping and selecting specific frequency bands.\n",
    "## Model Design:\n",
    "\n",
    "Created a base CNN with residual blocks for feature extraction.\n",
    "Integrated a Transformer encoder to capture temporal dependencies.\n",
    "Added a Bidirectional GRU layer for sequential modeling.\n",
    "Used a softmax output layer for classification into four emotion classes.\n",
    "\n",
    "## Training and Evaluation:\n",
    "\n",
    "Used K-Fold cross-validation to ensure robustness.\n",
    "Employed early stopping and learning rate reduction for efficient training.\n",
    "Evaluated model performance on both test and validation sets.\n",
    "\n",
    "## Results:\n",
    "\n",
    "Calculated mean and standard deviation of accuracies.\n",
    "Saved trained models for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b523248-421a-4c64-b935-db630da0902b",
   "metadata": {},
   "source": [
    "### Data Partitioning and Experimental Setup\n",
    "#### Training and Validation Data (Trials 1–16):\n",
    "\n",
    "- The EEG data from trials 1–16 is used exclusively for training and validation.\n",
    "  - To ensure robust model evaluation during the development phase, a 5-fold cross-validation (K-Fold) approach is applied:\n",
    "  - The trials 1–16 portion of the dataset is split into 5 subsets (folds).\n",
    "- For each fold:\n",
    "  - One subset is used as the validation set.\n",
    "  - The remaining four subsets are combined to form the training set.\n",
    "  - This process is repeated 5 times, each time with a different subset serving as the validation set.\n",
    "  - The goal of this step is to fine-tune hyperparameters, assess model stability, and reduce overfitting by examining performance across multiple splits.\n",
    "\n",
    "#### Dedicated Test Set (Trials 17–24):\n",
    "\n",
    "- After the model has been trained and validated using the 1–16 trial data, a completely separate set of trials (17–24) is used as a final test set.\n",
    "- This test set is not involved in any way during the training or validation phases.\n",
    "- Evaluating the model on trials 17–24 provides an unbiased estimate of the model’s generalization performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0262609-41db-43a4-81bf-923226da74ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "one_y categorical shape-(34800, 4)\n",
      "one_y categorical shape-(15420, 4)\n",
      "Epoch 1/150\n",
      "153/153 [==============================] - 16s 68ms/step - loss: 1.3026 - accuracy: 0.3975 - val_loss: 1.2538 - val_accuracy: 0.4368 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 1.1610 - accuracy: 0.4870 - val_loss: 1.1173 - val_accuracy: 0.5094 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 1.0676 - accuracy: 0.5380 - val_loss: 1.0155 - val_accuracy: 0.5782 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9845 - accuracy: 0.5835 - val_loss: 1.0226 - val_accuracy: 0.5675 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9185 - accuracy: 0.6170 - val_loss: 0.8932 - val_accuracy: 0.6361 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.8631 - accuracy: 0.6414 - val_loss: 0.9155 - val_accuracy: 0.6318 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.8036 - accuracy: 0.6720 - val_loss: 0.8916 - val_accuracy: 0.6379 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7651 - accuracy: 0.6901 - val_loss: 0.8352 - val_accuracy: 0.6527 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.7186 - accuracy: 0.7098 - val_loss: 0.7503 - val_accuracy: 0.7005 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6847 - accuracy: 0.7238 - val_loss: 0.7038 - val_accuracy: 0.7188 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6317 - accuracy: 0.7457 - val_loss: 0.7711 - val_accuracy: 0.6989 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6062 - accuracy: 0.7583 - val_loss: 0.6892 - val_accuracy: 0.7282 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5817 - accuracy: 0.7727 - val_loss: 0.6398 - val_accuracy: 0.7463 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5460 - accuracy: 0.7828 - val_loss: 0.6956 - val_accuracy: 0.7202 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5296 - accuracy: 0.7928 - val_loss: 0.7405 - val_accuracy: 0.7018 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4940 - accuracy: 0.8106 - val_loss: 0.6516 - val_accuracy: 0.7461 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4784 - accuracy: 0.8116 - val_loss: 0.6255 - val_accuracy: 0.7607 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4620 - accuracy: 0.8199 - val_loss: 0.6171 - val_accuracy: 0.7599 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4354 - accuracy: 0.8294 - val_loss: 0.5764 - val_accuracy: 0.7785 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4202 - accuracy: 0.8376 - val_loss: 0.5641 - val_accuracy: 0.7919 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3959 - accuracy: 0.8485 - val_loss: 0.5692 - val_accuracy: 0.7839 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3852 - accuracy: 0.8492 - val_loss: 0.5666 - val_accuracy: 0.7826 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3652 - accuracy: 0.8602 - val_loss: 0.5414 - val_accuracy: 0.8054 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3530 - accuracy: 0.8658 - val_loss: 0.5888 - val_accuracy: 0.7814 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3400 - accuracy: 0.8689 - val_loss: 0.5586 - val_accuracy: 0.7945 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3266 - accuracy: 0.8736 - val_loss: 0.6499 - val_accuracy: 0.7674 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3176 - accuracy: 0.8814 - val_loss: 0.5091 - val_accuracy: 0.8163 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3206 - accuracy: 0.8759 - val_loss: 0.5383 - val_accuracy: 0.8011 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2888 - accuracy: 0.8925 - val_loss: 0.5375 - val_accuracy: 0.8073 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.2764 - accuracy: 0.8936 - val_loss: 0.5320 - val_accuracy: 0.8036 - lr: 5.0000e-04\n",
      "Epoch 31/150\n",
      "153/153 [==============================] - 10s 63ms/step - loss: 0.2689 - accuracy: 0.8998 - val_loss: 0.5513 - val_accuracy: 0.8089 - lr: 5.0000e-04\n",
      "Epoch 32/150\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8958\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2714 - accuracy: 0.8958 - val_loss: 0.5471 - val_accuracy: 0.8052 - lr: 5.0000e-04\n",
      "Epoch 33/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1673 - accuracy: 0.9430 - val_loss: 0.3893 - val_accuracy: 0.8637 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1409 - accuracy: 0.9508 - val_loss: 0.3888 - val_accuracy: 0.8709 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1317 - accuracy: 0.9542 - val_loss: 0.3774 - val_accuracy: 0.8701 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1232 - accuracy: 0.9571 - val_loss: 0.3947 - val_accuracy: 0.8680 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1242 - accuracy: 0.9575 - val_loss: 0.4043 - val_accuracy: 0.8656 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1125 - accuracy: 0.9614 - val_loss: 0.4062 - val_accuracy: 0.8676 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1101 - accuracy: 0.9614 - val_loss: 0.4042 - val_accuracy: 0.8699 - lr: 1.0000e-04\n",
      "Epoch 40/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9631\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1059 - accuracy: 0.9631 - val_loss: 0.4065 - val_accuracy: 0.8709 - lr: 1.0000e-04\n",
      "Epoch 41/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0913 - accuracy: 0.9688 - val_loss: 0.3889 - val_accuracy: 0.8773 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0864 - accuracy: 0.9712 - val_loss: 0.3798 - val_accuracy: 0.8777 - lr: 2.0000e-05\n",
      "Epoch 43/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0876 - accuracy: 0.9699 - val_loss: 0.3887 - val_accuracy: 0.8787 - lr: 2.0000e-05\n",
      "Epoch 44/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0846 - accuracy: 0.9715 - val_loss: 0.3854 - val_accuracy: 0.8775 - lr: 2.0000e-05\n",
      "Epoch 45/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9744\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0811 - accuracy: 0.9743 - val_loss: 0.3867 - val_accuracy: 0.8783 - lr: 2.0000e-05\n",
      "Fold 1 - Test Accuracy: 87.01%, Validation Accuracy: 47.62%\n",
      "Epoch 1/150\n",
      "153/153 [==============================] - 16s 68ms/step - loss: 1.3057 - accuracy: 0.3924 - val_loss: 1.2822 - val_accuracy: 0.4487 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 1.1562 - accuracy: 0.4913 - val_loss: 1.1784 - val_accuracy: 0.4772 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 1.0731 - accuracy: 0.5389 - val_loss: 1.1295 - val_accuracy: 0.5018 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.9901 - accuracy: 0.5791 - val_loss: 1.0737 - val_accuracy: 0.5408 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9228 - accuracy: 0.6138 - val_loss: 0.9812 - val_accuracy: 0.5950 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.8548 - accuracy: 0.6477 - val_loss: 1.0590 - val_accuracy: 0.5581 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.8041 - accuracy: 0.6709 - val_loss: 0.8803 - val_accuracy: 0.6371 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.7670 - accuracy: 0.6856 - val_loss: 0.8182 - val_accuracy: 0.6706 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7177 - accuracy: 0.7101 - val_loss: 0.9101 - val_accuracy: 0.6342 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6818 - accuracy: 0.7281 - val_loss: 0.7855 - val_accuracy: 0.6775 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6430 - accuracy: 0.7446 - val_loss: 0.7319 - val_accuracy: 0.7128 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.6072 - accuracy: 0.7582 - val_loss: 0.7158 - val_accuracy: 0.7215 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5759 - accuracy: 0.7725 - val_loss: 0.7235 - val_accuracy: 0.7110 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5540 - accuracy: 0.7817 - val_loss: 0.7266 - val_accuracy: 0.7186 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5222 - accuracy: 0.7920 - val_loss: 0.8029 - val_accuracy: 0.6847 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4954 - accuracy: 0.8065 - val_loss: 0.5926 - val_accuracy: 0.7701 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.4690 - accuracy: 0.8189 - val_loss: 0.7359 - val_accuracy: 0.7217 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.4530 - accuracy: 0.8255 - val_loss: 0.6100 - val_accuracy: 0.7748 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4349 - accuracy: 0.8304 - val_loss: 0.5803 - val_accuracy: 0.7814 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4056 - accuracy: 0.8453 - val_loss: 0.5737 - val_accuracy: 0.7872 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3892 - accuracy: 0.8517 - val_loss: 0.6472 - val_accuracy: 0.7590 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3845 - accuracy: 0.8532 - val_loss: 0.5652 - val_accuracy: 0.7962 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.3590 - accuracy: 0.8631 - val_loss: 0.5494 - val_accuracy: 0.7943 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.3610 - accuracy: 0.8615 - val_loss: 0.5145 - val_accuracy: 0.8091 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.3395 - accuracy: 0.8715 - val_loss: 0.5864 - val_accuracy: 0.7816 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "153/153 [==============================] - 10s 63ms/step - loss: 0.3221 - accuracy: 0.8779 - val_loss: 0.5690 - val_accuracy: 0.7931 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3144 - accuracy: 0.8786 - val_loss: 0.5293 - val_accuracy: 0.8083 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.3031 - accuracy: 0.8841 - val_loss: 0.5670 - val_accuracy: 0.7894 - lr: 5.0000e-04\n",
      "Epoch 29/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.2993 - accuracy: 0.8862\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2994 - accuracy: 0.8860 - val_loss: 0.5759 - val_accuracy: 0.7939 - lr: 5.0000e-04\n",
      "Epoch 30/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1901 - accuracy: 0.9302 - val_loss: 0.3866 - val_accuracy: 0.8662 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "153/153 [==============================] - 10s 64ms/step - loss: 0.1552 - accuracy: 0.9462 - val_loss: 0.3901 - val_accuracy: 0.8643 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "153/153 [==============================] - 10s 64ms/step - loss: 0.1446 - accuracy: 0.9501 - val_loss: 0.3942 - val_accuracy: 0.8664 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "153/153 [==============================] - 10s 64ms/step - loss: 0.1371 - accuracy: 0.9523 - val_loss: 0.3826 - val_accuracy: 0.8664 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "153/153 [==============================] - 10s 63ms/step - loss: 0.1312 - accuracy: 0.9546 - val_loss: 0.3890 - val_accuracy: 0.8688 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1290 - accuracy: 0.9546 - val_loss: 0.3845 - val_accuracy: 0.8705 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1231 - accuracy: 0.9561 - val_loss: 0.3920 - val_accuracy: 0.8672 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1157 - accuracy: 0.9595 - val_loss: 0.4000 - val_accuracy: 0.8654 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9607\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1152 - accuracy: 0.9607 - val_loss: 0.3973 - val_accuracy: 0.8705 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0986 - accuracy: 0.9667 - val_loss: 0.3818 - val_accuracy: 0.8729 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0942 - accuracy: 0.9684 - val_loss: 0.3814 - val_accuracy: 0.8754 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0917 - accuracy: 0.9688 - val_loss: 0.3866 - val_accuracy: 0.8740 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0919 - accuracy: 0.9693 - val_loss: 0.3821 - val_accuracy: 0.8762 - lr: 2.0000e-05\n",
      "Epoch 43/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0891 - accuracy: 0.9693 - val_loss: 0.3831 - val_accuracy: 0.8750 - lr: 2.0000e-05\n",
      "Epoch 44/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0864 - accuracy: 0.9725 - val_loss: 0.3841 - val_accuracy: 0.8738 - lr: 2.0000e-05\n",
      "Epoch 45/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9715\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0879 - accuracy: 0.9715 - val_loss: 0.3817 - val_accuracy: 0.8764 - lr: 2.0000e-05\n",
      "Epoch 46/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0859 - accuracy: 0.9717 - val_loss: 0.3819 - val_accuracy: 0.8781 - lr: 4.0000e-06\n",
      "Epoch 47/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0809 - accuracy: 0.9737 - val_loss: 0.3791 - val_accuracy: 0.8783 - lr: 4.0000e-06\n",
      "Epoch 48/150\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.0841 - accuracy: 0.9720 - val_loss: 0.3786 - val_accuracy: 0.8773 - lr: 4.0000e-06\n",
      "Epoch 49/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0819 - accuracy: 0.9744 - val_loss: 0.3815 - val_accuracy: 0.8775 - lr: 4.0000e-06\n",
      "Epoch 50/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0815 - accuracy: 0.9723 - val_loss: 0.3766 - val_accuracy: 0.8783 - lr: 4.0000e-06\n",
      "Epoch 51/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0811 - accuracy: 0.9740 - val_loss: 0.3796 - val_accuracy: 0.8777 - lr: 4.0000e-06\n",
      "Epoch 52/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0806 - accuracy: 0.9733 - val_loss: 0.3800 - val_accuracy: 0.8793 - lr: 4.0000e-06\n",
      "Epoch 53/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0809 - accuracy: 0.9742 - val_loss: 0.3765 - val_accuracy: 0.8783 - lr: 4.0000e-06\n",
      "Epoch 54/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0826 - accuracy: 0.9737 - val_loss: 0.3774 - val_accuracy: 0.8779 - lr: 4.0000e-06\n",
      "Epoch 55/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0822 - accuracy: 0.9737 - val_loss: 0.3791 - val_accuracy: 0.8766 - lr: 4.0000e-06\n",
      "Epoch 56/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0757 - accuracy: 0.9772 - val_loss: 0.3810 - val_accuracy: 0.8746 - lr: 4.0000e-06\n",
      "Epoch 57/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0802 - accuracy: 0.9731 - val_loss: 0.3823 - val_accuracy: 0.8777 - lr: 4.0000e-06\n",
      "Epoch 58/150\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9730\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0825 - accuracy: 0.9730 - val_loss: 0.3802 - val_accuracy: 0.8773 - lr: 4.0000e-06\n",
      "Epoch 59/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0796 - accuracy: 0.9737 - val_loss: 0.3828 - val_accuracy: 0.8777 - lr: 1.0000e-06\n",
      "Epoch 60/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0799 - accuracy: 0.9740 - val_loss: 0.3803 - val_accuracy: 0.8766 - lr: 1.0000e-06\n",
      "Epoch 61/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0807 - accuracy: 0.9738 - val_loss: 0.3807 - val_accuracy: 0.8795 - lr: 1.0000e-06\n",
      "Epoch 62/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0779 - accuracy: 0.9759 - val_loss: 0.3795 - val_accuracy: 0.8785 - lr: 1.0000e-06\n",
      "Epoch 63/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.0791 - accuracy: 0.9739 - val_loss: 0.3784 - val_accuracy: 0.8781 - lr: 1.0000e-06\n",
      "Fold 2 - Test Accuracy: 87.50%, Validation Accuracy: 47.34%\n",
      "Epoch 1/150\n",
      "153/153 [==============================] - 16s 69ms/step - loss: 1.3029 - accuracy: 0.3901 - val_loss: 1.4044 - val_accuracy: 0.3832 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 1.1575 - accuracy: 0.4887 - val_loss: 1.2520 - val_accuracy: 0.4641 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 1.0536 - accuracy: 0.5484 - val_loss: 1.0996 - val_accuracy: 0.5503 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.9727 - accuracy: 0.5871 - val_loss: 0.9766 - val_accuracy: 0.5872 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9042 - accuracy: 0.6234 - val_loss: 0.9692 - val_accuracy: 0.5952 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.8415 - accuracy: 0.6552 - val_loss: 0.8510 - val_accuracy: 0.6453 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7871 - accuracy: 0.6832 - val_loss: 0.9115 - val_accuracy: 0.6160 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7386 - accuracy: 0.7022 - val_loss: 0.8985 - val_accuracy: 0.6379 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7050 - accuracy: 0.7149 - val_loss: 0.8440 - val_accuracy: 0.6470 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6533 - accuracy: 0.7410 - val_loss: 0.9228 - val_accuracy: 0.6297 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.6286 - accuracy: 0.7477 - val_loss: 0.7048 - val_accuracy: 0.7155 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5880 - accuracy: 0.7638 - val_loss: 0.6515 - val_accuracy: 0.7401 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5631 - accuracy: 0.7779 - val_loss: 0.7811 - val_accuracy: 0.6890 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5361 - accuracy: 0.7881 - val_loss: 0.6463 - val_accuracy: 0.7457 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5025 - accuracy: 0.8045 - val_loss: 0.6636 - val_accuracy: 0.7418 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.4857 - accuracy: 0.8094 - val_loss: 0.5836 - val_accuracy: 0.7672 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4587 - accuracy: 0.8225 - val_loss: 0.6502 - val_accuracy: 0.7504 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4399 - accuracy: 0.8277 - val_loss: 0.6332 - val_accuracy: 0.7572 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4299 - accuracy: 0.8312 - val_loss: 0.6237 - val_accuracy: 0.7644 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4155 - accuracy: 0.8411 - val_loss: 0.5476 - val_accuracy: 0.7933 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3860 - accuracy: 0.8504 - val_loss: 0.5481 - val_accuracy: 0.7956 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3741 - accuracy: 0.8553 - val_loss: 0.5479 - val_accuracy: 0.7972 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3645 - accuracy: 0.8601 - val_loss: 0.5543 - val_accuracy: 0.7898 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.3485 - accuracy: 0.8622 - val_loss: 0.6040 - val_accuracy: 0.7705 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.8733\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3311 - accuracy: 0.8732 - val_loss: 0.6093 - val_accuracy: 0.7769 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2169 - accuracy: 0.9221 - val_loss: 0.4212 - val_accuracy: 0.8463 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1849 - accuracy: 0.9349 - val_loss: 0.4243 - val_accuracy: 0.8454 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1748 - accuracy: 0.9378 - val_loss: 0.4089 - val_accuracy: 0.8543 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1691 - accuracy: 0.9413 - val_loss: 0.4135 - val_accuracy: 0.8582 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1591 - accuracy: 0.9439 - val_loss: 0.4117 - val_accuracy: 0.8573 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1554 - accuracy: 0.9455 - val_loss: 0.4324 - val_accuracy: 0.8495 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1542 - accuracy: 0.9460 - val_loss: 0.4060 - val_accuracy: 0.8623 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1497 - accuracy: 0.9468 - val_loss: 0.4189 - val_accuracy: 0.8578 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1429 - accuracy: 0.9492 - val_loss: 0.4221 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1395 - accuracy: 0.9496 - val_loss: 0.4438 - val_accuracy: 0.8549 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1355 - accuracy: 0.9496 - val_loss: 0.4249 - val_accuracy: 0.8654 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9534\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1308 - accuracy: 0.9533 - val_loss: 0.4273 - val_accuracy: 0.8580 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1111 - accuracy: 0.9622 - val_loss: 0.4041 - val_accuracy: 0.8668 - lr: 2.0000e-05\n",
      "Epoch 39/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1022 - accuracy: 0.9649 - val_loss: 0.4061 - val_accuracy: 0.8672 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1039 - accuracy: 0.9650 - val_loss: 0.4077 - val_accuracy: 0.8674 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1057 - accuracy: 0.9631 - val_loss: 0.4068 - val_accuracy: 0.8676 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1008 - accuracy: 0.9658 - val_loss: 0.4100 - val_accuracy: 0.8666 - lr: 2.0000e-05\n",
      "Epoch 43/150\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9675\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.0970 - accuracy: 0.9675 - val_loss: 0.4170 - val_accuracy: 0.8668 - lr: 2.0000e-05\n",
      "Epoch 44/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0931 - accuracy: 0.9691 - val_loss: 0.4141 - val_accuracy: 0.8684 - lr: 4.0000e-06\n",
      "Epoch 45/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0952 - accuracy: 0.9685 - val_loss: 0.4117 - val_accuracy: 0.8705 - lr: 4.0000e-06\n",
      "Epoch 46/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0902 - accuracy: 0.9702 - val_loss: 0.4124 - val_accuracy: 0.8676 - lr: 4.0000e-06\n",
      "Epoch 47/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.0915 - accuracy: 0.9698 - val_loss: 0.4103 - val_accuracy: 0.8699 - lr: 4.0000e-06\n",
      "Epoch 48/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9694\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.0897 - accuracy: 0.9694 - val_loss: 0.4153 - val_accuracy: 0.8674 - lr: 4.0000e-06\n",
      "Fold 3 - Test Accuracy: 87.41%, Validation Accuracy: 48.29%\n",
      "Epoch 1/150\n",
      "153/153 [==============================] - 16s 67ms/step - loss: 1.3196 - accuracy: 0.3837 - val_loss: 1.3423 - val_accuracy: 0.4440 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 1.1685 - accuracy: 0.4826 - val_loss: 1.1431 - val_accuracy: 0.5131 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 1.0578 - accuracy: 0.5482 - val_loss: 1.0552 - val_accuracy: 0.5538 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9836 - accuracy: 0.5851 - val_loss: 1.0316 - val_accuracy: 0.5538 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9124 - accuracy: 0.6169 - val_loss: 0.9057 - val_accuracy: 0.6223 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.8494 - accuracy: 0.6527 - val_loss: 0.8678 - val_accuracy: 0.6463 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.7896 - accuracy: 0.6781 - val_loss: 0.8637 - val_accuracy: 0.6424 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7448 - accuracy: 0.6979 - val_loss: 0.8043 - val_accuracy: 0.6739 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7100 - accuracy: 0.7144 - val_loss: 0.9143 - val_accuracy: 0.6459 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6645 - accuracy: 0.7344 - val_loss: 0.7020 - val_accuracy: 0.7194 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.6248 - accuracy: 0.7508 - val_loss: 0.6910 - val_accuracy: 0.7219 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.6014 - accuracy: 0.7627 - val_loss: 0.7532 - val_accuracy: 0.7087 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5607 - accuracy: 0.7777 - val_loss: 0.6933 - val_accuracy: 0.7233 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5323 - accuracy: 0.7935 - val_loss: 0.6883 - val_accuracy: 0.7264 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5126 - accuracy: 0.7995 - val_loss: 0.6045 - val_accuracy: 0.7644 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4895 - accuracy: 0.8074 - val_loss: 0.5875 - val_accuracy: 0.7613 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4587 - accuracy: 0.8205 - val_loss: 0.6412 - val_accuracy: 0.7537 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4486 - accuracy: 0.8265 - val_loss: 0.5656 - val_accuracy: 0.7919 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "153/153 [==============================] - 10s 63ms/step - loss: 0.4302 - accuracy: 0.8350 - val_loss: 0.5204 - val_accuracy: 0.7966 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "153/153 [==============================] - 10s 64ms/step - loss: 0.4125 - accuracy: 0.8405 - val_loss: 0.6512 - val_accuracy: 0.7461 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3974 - accuracy: 0.8449 - val_loss: 0.5975 - val_accuracy: 0.7794 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3759 - accuracy: 0.8552 - val_loss: 0.5659 - val_accuracy: 0.7808 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3715 - accuracy: 0.8596 - val_loss: 0.6090 - val_accuracy: 0.7691 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.3419 - accuracy: 0.8685\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3421 - accuracy: 0.8684 - val_loss: 0.6302 - val_accuracy: 0.7695 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2373 - accuracy: 0.9135 - val_loss: 0.4030 - val_accuracy: 0.8541 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2066 - accuracy: 0.9253 - val_loss: 0.3870 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1897 - accuracy: 0.9333 - val_loss: 0.4054 - val_accuracy: 0.8551 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1846 - accuracy: 0.9329 - val_loss: 0.3981 - val_accuracy: 0.8573 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1727 - accuracy: 0.9382 - val_loss: 0.4229 - val_accuracy: 0.8520 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1746 - accuracy: 0.9361 - val_loss: 0.4098 - val_accuracy: 0.8578 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1657 - accuracy: 0.9389\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1658 - accuracy: 0.9389 - val_loss: 0.4120 - val_accuracy: 0.8621 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1428 - accuracy: 0.9520 - val_loss: 0.3857 - val_accuracy: 0.8660 - lr: 2.0000e-05\n",
      "Epoch 33/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1383 - accuracy: 0.9522 - val_loss: 0.3809 - val_accuracy: 0.8693 - lr: 2.0000e-05\n",
      "Epoch 34/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1352 - accuracy: 0.9520 - val_loss: 0.3813 - val_accuracy: 0.8713 - lr: 2.0000e-05\n",
      "Epoch 35/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1364 - accuracy: 0.9541 - val_loss: 0.3910 - val_accuracy: 0.8658 - lr: 2.0000e-05\n",
      "Epoch 36/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1338 - accuracy: 0.9534 - val_loss: 0.3797 - val_accuracy: 0.8693 - lr: 2.0000e-05\n",
      "Epoch 37/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1321 - accuracy: 0.9548 - val_loss: 0.3897 - val_accuracy: 0.8688 - lr: 2.0000e-05\n",
      "Epoch 38/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1286 - accuracy: 0.9557 - val_loss: 0.3889 - val_accuracy: 0.8686 - lr: 2.0000e-05\n",
      "Epoch 39/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1315 - accuracy: 0.9529 - val_loss: 0.3859 - val_accuracy: 0.8705 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1285 - accuracy: 0.9552 - val_loss: 0.3856 - val_accuracy: 0.8693 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1238 - accuracy: 0.9570\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1243 - accuracy: 0.9569 - val_loss: 0.3800 - val_accuracy: 0.8742 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1214 - accuracy: 0.9585 - val_loss: 0.3827 - val_accuracy: 0.8709 - lr: 4.0000e-06\n",
      "Epoch 43/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1237 - accuracy: 0.9567 - val_loss: 0.3918 - val_accuracy: 0.8701 - lr: 4.0000e-06\n",
      "Epoch 44/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1237 - accuracy: 0.9578 - val_loss: 0.3850 - val_accuracy: 0.8703 - lr: 4.0000e-06\n",
      "Epoch 45/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1157 - accuracy: 0.9608 - val_loss: 0.3843 - val_accuracy: 0.8705 - lr: 4.0000e-06\n",
      "Epoch 46/150\n",
      "153/153 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9603\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1164 - accuracy: 0.9603 - val_loss: 0.3835 - val_accuracy: 0.8725 - lr: 4.0000e-06\n",
      "Fold 4 - Test Accuracy: 87.52%, Validation Accuracy: 48.68%\n",
      "Epoch 1/150\n",
      "153/153 [==============================] - 15s 67ms/step - loss: 1.3123 - accuracy: 0.3862 - val_loss: 1.3518 - val_accuracy: 0.4446 - lr: 5.0000e-04\n",
      "Epoch 2/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 1.1593 - accuracy: 0.4900 - val_loss: 1.2129 - val_accuracy: 0.4772 - lr: 5.0000e-04\n",
      "Epoch 3/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 1.0531 - accuracy: 0.5484 - val_loss: 1.1798 - val_accuracy: 0.5006 - lr: 5.0000e-04\n",
      "Epoch 4/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.9699 - accuracy: 0.5932 - val_loss: 0.9937 - val_accuracy: 0.5864 - lr: 5.0000e-04\n",
      "Epoch 5/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.9054 - accuracy: 0.6240 - val_loss: 1.0783 - val_accuracy: 0.5503 - lr: 5.0000e-04\n",
      "Epoch 6/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.8553 - accuracy: 0.6463 - val_loss: 0.8727 - val_accuracy: 0.6492 - lr: 5.0000e-04\n",
      "Epoch 7/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7940 - accuracy: 0.6766 - val_loss: 0.8811 - val_accuracy: 0.6427 - lr: 5.0000e-04\n",
      "Epoch 8/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.7380 - accuracy: 0.7019 - val_loss: 0.9004 - val_accuracy: 0.6342 - lr: 5.0000e-04\n",
      "Epoch 9/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.7067 - accuracy: 0.7163 - val_loss: 0.7948 - val_accuracy: 0.6802 - lr: 5.0000e-04\n",
      "Epoch 10/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.6667 - accuracy: 0.7312 - val_loss: 0.7809 - val_accuracy: 0.6907 - lr: 5.0000e-04\n",
      "Epoch 11/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.6223 - accuracy: 0.7541 - val_loss: 0.7149 - val_accuracy: 0.7131 - lr: 5.0000e-04\n",
      "Epoch 12/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.5946 - accuracy: 0.7646 - val_loss: 0.6892 - val_accuracy: 0.7260 - lr: 5.0000e-04\n",
      "Epoch 13/150\n",
      "153/153 [==============================] - 10s 62ms/step - loss: 0.5639 - accuracy: 0.7767 - val_loss: 0.6218 - val_accuracy: 0.7564 - lr: 5.0000e-04\n",
      "Epoch 14/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.5338 - accuracy: 0.7881 - val_loss: 0.7053 - val_accuracy: 0.7291 - lr: 5.0000e-04\n",
      "Epoch 15/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.5126 - accuracy: 0.7998 - val_loss: 0.6510 - val_accuracy: 0.7443 - lr: 5.0000e-04\n",
      "Epoch 16/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.4804 - accuracy: 0.8151 - val_loss: 0.5899 - val_accuracy: 0.7722 - lr: 5.0000e-04\n",
      "Epoch 17/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4601 - accuracy: 0.8199 - val_loss: 0.5813 - val_accuracy: 0.7757 - lr: 5.0000e-04\n",
      "Epoch 18/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.4366 - accuracy: 0.8288 - val_loss: 0.7359 - val_accuracy: 0.7258 - lr: 5.0000e-04\n",
      "Epoch 19/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.4294 - accuracy: 0.8350 - val_loss: 0.6786 - val_accuracy: 0.7459 - lr: 5.0000e-04\n",
      "Epoch 20/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.3978 - accuracy: 0.8445 - val_loss: 0.6102 - val_accuracy: 0.7738 - lr: 5.0000e-04\n",
      "Epoch 21/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.3931 - accuracy: 0.8483 - val_loss: 0.5380 - val_accuracy: 0.7970 - lr: 5.0000e-04\n",
      "Epoch 22/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3807 - accuracy: 0.8537 - val_loss: 0.5370 - val_accuracy: 0.7952 - lr: 5.0000e-04\n",
      "Epoch 23/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3546 - accuracy: 0.8644 - val_loss: 0.6588 - val_accuracy: 0.7547 - lr: 5.0000e-04\n",
      "Epoch 24/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.3443 - accuracy: 0.8665 - val_loss: 0.5378 - val_accuracy: 0.8050 - lr: 5.0000e-04\n",
      "Epoch 25/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3314 - accuracy: 0.8737 - val_loss: 0.6407 - val_accuracy: 0.7683 - lr: 5.0000e-04\n",
      "Epoch 26/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.3125 - accuracy: 0.8803 - val_loss: 0.5989 - val_accuracy: 0.7904 - lr: 5.0000e-04\n",
      "Epoch 27/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.8792\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.3138 - accuracy: 0.8792 - val_loss: 0.5772 - val_accuracy: 0.7919 - lr: 5.0000e-04\n",
      "Epoch 28/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.2008 - accuracy: 0.9275 - val_loss: 0.4059 - val_accuracy: 0.8615 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1638 - accuracy: 0.9420 - val_loss: 0.4157 - val_accuracy: 0.8573 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "153/153 [==============================] - 12s 78ms/step - loss: 0.1584 - accuracy: 0.9452 - val_loss: 0.4090 - val_accuracy: 0.8598 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "153/153 [==============================] - 15s 97ms/step - loss: 0.1536 - accuracy: 0.9461 - val_loss: 0.4105 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "153/153 [==============================] - 16s 105ms/step - loss: 0.1465 - accuracy: 0.9480 - val_loss: 0.4249 - val_accuracy: 0.8563 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1385 - accuracy: 0.9515\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "153/153 [==============================] - 13s 85ms/step - loss: 0.1385 - accuracy: 0.9515 - val_loss: 0.4100 - val_accuracy: 0.8631 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1180 - accuracy: 0.9609 - val_loss: 0.4073 - val_accuracy: 0.8645 - lr: 2.0000e-05\n",
      "Epoch 35/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1181 - accuracy: 0.9606 - val_loss: 0.4051 - val_accuracy: 0.8672 - lr: 2.0000e-05\n",
      "Epoch 36/150\n",
      "153/153 [==============================] - 9s 62ms/step - loss: 0.1145 - accuracy: 0.9618 - val_loss: 0.4025 - val_accuracy: 0.8676 - lr: 2.0000e-05\n",
      "Epoch 37/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1134 - accuracy: 0.9619 - val_loss: 0.4032 - val_accuracy: 0.8686 - lr: 2.0000e-05\n",
      "Epoch 38/150\n",
      "153/153 [==============================] - 11s 73ms/step - loss: 0.1126 - accuracy: 0.9624 - val_loss: 0.4025 - val_accuracy: 0.8651 - lr: 2.0000e-05\n",
      "Epoch 39/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1101 - accuracy: 0.9637 - val_loss: 0.4053 - val_accuracy: 0.8678 - lr: 2.0000e-05\n",
      "Epoch 40/150\n",
      "153/153 [==============================] - 9s 59ms/step - loss: 0.1104 - accuracy: 0.9623 - val_loss: 0.4049 - val_accuracy: 0.8684 - lr: 2.0000e-05\n",
      "Epoch 41/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9639\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1091 - accuracy: 0.9635 - val_loss: 0.4045 - val_accuracy: 0.8703 - lr: 2.0000e-05\n",
      "Epoch 42/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1051 - accuracy: 0.9661 - val_loss: 0.4058 - val_accuracy: 0.8674 - lr: 4.0000e-06\n",
      "Epoch 43/150\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1006 - accuracy: 0.9685 - val_loss: 0.4081 - val_accuracy: 0.8684 - lr: 4.0000e-06\n",
      "Epoch 44/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1052 - accuracy: 0.9642 - val_loss: 0.4075 - val_accuracy: 0.8674 - lr: 4.0000e-06\n",
      "Epoch 45/150\n",
      "153/153 [==============================] - 9s 61ms/step - loss: 0.1052 - accuracy: 0.9652 - val_loss: 0.4067 - val_accuracy: 0.8682 - lr: 4.0000e-06\n",
      "Epoch 46/150\n",
      "152/153 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9646\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "153/153 [==============================] - 9s 60ms/step - loss: 0.1038 - accuracy: 0.9646 - val_loss: 0.4066 - val_accuracy: 0.8678 - lr: 4.0000e-06\n",
      "Fold 5 - Test Accuracy: 86.83%, Validation Accuracy: 48.72%\n",
      "acc: [87.01149225234985, 87.5, 87.41379380226135, 87.51915693283081, 86.82950139045715]\n",
      "mean acc: 87.25478887557983\n",
      "std acc: 0.28119894161256664\n",
      "val: [47.6199746131897, 47.34111428260803, 48.294422030448914, 48.68352711200714, 48.715952038764954]\n",
      "mean vali: 48.13099801540375\n",
      "std vali: 0.5584314191875352\n",
      "2415.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Concatenate, Reshape, BatchNormalization, Bidirectional, GRU, Add, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "from keras.models import  Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout,Layer,Lambda\n",
    "from keras.layers import Flatten, Dense, Concatenate, Reshape, LSTM,BatchNormalization,Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "from keras import backend as K\n",
    "import time\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "import datetime\n",
    "\n",
    "# Set GPU configuration\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Data Loading and Reshaping\n",
    "num_classes = 4\n",
    "batch_size = 128\n",
    "img_rows, img_cols, num_chan = 8, 9, 4\n",
    "\n",
    "#==============================load data=================================================\n",
    "\n",
    "x_89_Seg_1_16 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t6x_89_Seg_1_16.npy\")\n",
    "y_89_Seg_1_16 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t6y_89_Seg_1_16.npy\")\n",
    "x_89_Seg_17_24 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t6x_89_Seg_17_24.npy\")\n",
    "y_89_Seg_17_24 = np.load(\"D:/BigData/SEED_IV/SEED_IV/DE0.5s/session1_2_3_exp1/t6y_89_Seg_17_24.npy\")\n",
    "\n",
    "def normalization(y,falx):\n",
    "    one_y = to_categorical(y, num_classes)\n",
    "    print('{}-{}'.format('one_y categorical shape', one_y.shape))\n",
    "\n",
    "    one_falx_1 = falx.reshape((-1, 6, img_rows, img_cols, 5))  # reshape each person's segments\n",
    "    one_falx = one_falx_1[:, :, :, :, 1:5]  # only 4 bands since last band reflects sleep feature\n",
    "    return one_falx, one_y\n",
    "\n",
    "one_x_1_16, one_y_1_16 = normalization(y_89_Seg_1_16,x_89_Seg_1_16)\n",
    "one_x_17_24, one_y_17_24 = normalization(y_89_Seg_17_24,x_89_Seg_17_24)\n",
    "\n",
    "#======================model=============================================================\n",
    "# Base CNN Network with Residual Blocks\n",
    "def create_base_network(input_dim):\n",
    "    input_layer = Input(shape=input_dim)\n",
    "    x = Conv2D(32, 5, activation='relu', padding='same', name='conv1')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # First Residual Block\n",
    "    residual = Conv2D(128, 4, activation='relu', padding='same', name='conv2')(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "    residual = Dropout(0.2)(residual)\n",
    "    x = Conv2D(128, 4, activation='relu', padding='same', name='conv2_residual')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Add()([x, residual])\n",
    "    \n",
    "    # Second Residual Block\n",
    "    residual = Conv2D(256, 4, activation='relu', padding='same', name='conv3')(x)\n",
    "    residual = BatchNormalization()(residual)\n",
    "    residual = Dropout(0.2)(residual)\n",
    "    x = Conv2D(256, 4, activation='relu', padding='same', name='conv3_residual')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Add()([x, residual])\n",
    "    \n",
    "    x = Conv2D(64, 1, activation='relu', padding='same', name='conv4')(x)\n",
    "    x = MaxPooling2D(2, 2, name='pool1')(x)\n",
    "    x = Flatten(name='fla1')(x)\n",
    "    x = Dense(256, activation='relu', name='dense1')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Reshape((1, 256), name='reshape')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Vision Transformer Encoder Block\n",
    "def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    # Multi-Head Self Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn_output = Dropout(rate)(attn_output)\n",
    "    # Add & Norm\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    # Feed Forward Network\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dense(embed_dim)(ffn_output)\n",
    "    ffn_output = Dropout(rate)(ffn_output)\n",
    "    # Add & Norm\n",
    "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "\n",
    "# Training and Evaluation\n",
    "\n",
    "all_acc = []\n",
    "all_val= []\n",
    "\n",
    "# Initialize KFold with 5 splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "K.clear_session()\n",
    "start = time.time()\n",
    "\n",
    "# Use one_x_17_24 and one_y_17_24 as the validation set\n",
    "X_val, y_val = one_x_17_24, one_y_17_24\n",
    "\n",
    "# Split one_x_1_16 and one_y_1_16 into train and test sets (7:3 ratio)\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(one_x_1_16, one_y_1_16, test_size=0.3, random_state=42, stratify=one_y_1_16.argmax(1))\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train_all)):\n",
    "    # Split into current fold's train and validation sets\n",
    "    X_fold_train, X_fold_val = X_train_all[train_indices], X_train_all[val_indices]\n",
    "    y_fold_train, y_fold_val = y_train_all[train_indices], y_train_all[val_indices]\n",
    "\n",
    "    img_size = (img_rows, img_cols, num_chan)\n",
    "\n",
    "    # Create base network\n",
    "    base_network = create_base_network(img_size)\n",
    "                        \n",
    "    inputs = [Input(shape=img_size) for _ in range(6)]\n",
    "    out_all = Concatenate(axis=1)([base_network(inp) for inp in inputs])\n",
    "\n",
    "    # Transformer Encoder Block\n",
    "    embed_dim = 256\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    transformer_output = transformer_encoder(out_all, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "    # Bidirectional GRU Layer\n",
    "    bidir_gru = Bidirectional(GRU(128, return_sequences=False))(transformer_output)\n",
    "\n",
    "    # Output Layer\n",
    "    out_layer = Dense(4, activation='softmax', name='out')(bidir_gru)\n",
    "    model = Model(inputs, out_layer)\n",
    "\n",
    "    #tensorboard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                      optimizer=keras.optimizers.adam_v2.Adam(learning_rate=0.0005),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    #tensorboard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(\n",
    "    [X_fold_train[:, i, :, :, :] for i in range(6)],\n",
    "    y_fold_train,\n",
    "    epochs=150,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    validation_data=([X_fold_val[:, i, :, :, :] for i in range(6)], y_fold_val),\n",
    "    callbacks=[early_stopping, reduce_lr, tensorboard_callback])\n",
    "\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    scores_test = model.evaluate(\n",
    "    [X_test[:, i, :, :, :] for i in range(6)],\n",
    "    y_test,\n",
    "    verbose=0)\n",
    "    \n",
    "    # Evaluate the model on the dedicated validation set\n",
    "    scores_val = model.evaluate(\n",
    "    [X_val[:, i, :, :, :] for i in range(6)],\n",
    "    y_val,\n",
    "    verbose=0)\n",
    "    \n",
    "    #model.save(f'ResCNN-TransGRU_fold{fold + 1}.h5')\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - Test Accuracy: {scores_test[1] * 100:.2f}%, Validation Accuracy: {scores_val[1] * 100:.2f}%\")\n",
    "    all_acc.append(scores_test[1] * 100)\n",
    "    all_val.append(scores_val[1] * 100)\n",
    "    \n",
    "print(\"acc: {}\".format(all_acc))\n",
    "print(\"mean acc: {}\".format(np.mean(all_acc)))\n",
    "print(\"std acc: {}\".format(np.std(all_acc)))\n",
    "\n",
    "print(\"val: {}\".format(all_val))\n",
    "print(\"mean vali: {}\".format(np.mean(all_val)))\n",
    "print(\"std vali: {}\".format(np.std(all_val)))\n",
    "\n",
    "end = time.time()\n",
    "print(\"%.2f\" % (end - start))  # Run time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c07379-3aa9-45f6-9e20-bc4f752a242c",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546ad96-f82a-44d6-982f-2ca5c51fa9e1",
   "metadata": {},
   "source": [
    "acc: [87.01149225234985, 87.5, 87.41379380226135, 87.51915693283081, 86.82950139045715]\n",
    "- mean acc: 87.25478887557983\n",
    "- std acc: 0.28119894161256664\n",
    "\n",
    "val: [47.6199746131897, 47.34111428260803, 48.294422030448914, 48.68352711200714, 48.715952038764954]\n",
    "- mean vali: 48.13099801540375\n",
    "- std vali: 0.5584314191875352\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fe8b85-5273-4181-9a57-2a792a943fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
